{"cells":[{"cell_type":"markdown","source":["# Dataset Reduction\n"],"metadata":{"id":"Np3cP75ZM0dH"}},{"cell_type":"code","source":["# Parametri per l'elaborazione in chunk\n","CHUNK_SIZE = 10_000_000                     # Dimensione del singolo chunk\n","NORMAL_ROWS_RATIO = 1_000                   # Rapporto di campionamento delle righe normali\n","NORMAL_ROWS_SAMPLING_RATE_FALLBACK = 0.0001 # Rapporto di campionamento delle righe normali per chunk senza anomalie"],"metadata":{"id":"mCiXudV6Qwa3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Caricamento, preparazione e ispezione dei dati"],"metadata":{"id":"75ySEByfM92a"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Montaggio Google Drive\n","from google.colab import drive\n","\n","# Definizione dei percorsi\n","drive.mount('/content/drive', force_remount=True)\n","base_path = '/content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV'\n","dataset_path = f'{base_path}/Dataset'\n","auth_file_path = f'{dataset_path}/auth.txt.gz'\n","redteam_file_path = f'{dataset_path}/redteam.txt.gz'\n","\n","# Nomi delle colonne\n","# Colonne di auth.txt (9)\n","auth_cols = [\n","    'time',             # Tempo dell'evento (intero)\n","    'src_user',         # Utente sorgente (stringa, es. User1@DOM1)\n","    'dst_user',         # Utente destinazione (stringa, es. User2@DOM1 o Computer1$@DOM1)\n","    'src_comp',         # Computer sorgente (stringa, es. C1)\n","    'dst_comp',         # Computer destinazione (stringa, es. C2)\n","    'auth_type',        # Tipo di autenticazione (stringa, es. Kerberos, NTLM, Negotiate)\n","    'logon_type',       # Tipo di logon (stringa, es. Network, Interactive, Batch)\n","    'auth_orientation', # Orientamento dell'autenticazione (stringa, es. LogOn, LogOff, ...)\n","    'status'            # Esito dell'autenticazione (stringa, es. Success, Failure, ...)\n","]\n","print(f\"auth.txt columns: {auth_cols}\\n\")\n","# Colonne di redteam.txt (4)\n","redteam_cols = [\n","    'time_rt',          # Tempo dell'evento (intero)\n","    'src_user_rt',      # Utente sorgente coinvolto nell'attività del red team\n","    'src_comp_rt',      # Computer sorgente coinvolto nell'attività del red team\n","    'dst_comp_rt'       # Computer destinazione coinvolto nell'attività del red team\n","]\n","print(f\"redteam.txt columns: {redteam_cols}\\n\")"],"metadata":{"id":"CYhQ-BUzJT2x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Caricamento e definizione chiavi per redteam.txt.gz"],"metadata":{"id":"Y4MzFMaLx_cq"}},{"cell_type":"code","source":["import random\n","import pandas as pd\n","\n","# Lettura del DataFrame redteam_df\n","print(f\"Caricamento di {redteam_file_path}...\")\n","redteam_df = pd.read_csv(\n","    redteam_file_path,\n","    header=None,\n","    names=redteam_cols,\n","    sep=',',\n","    compression='gzip'\n",")\n","print(f\"Caricato redteam_df\")\n","print(f\"Righe: {redteam_df.shape[0]}\")\n","print(f\"Colonne: {redteam_df.shape[1]}\\n\")\n","\n","# Crea un set di chiavi dagli eventi del redteam per il lookup\n","print(\"Creazione del set di chiavi anomale da redteam_df...\")\n","redteam_event_keys = set()\n","# Iterazione su ogni riga del DataFrame:\n","# per ogni riga di redteam si estraggono\n","# i dati e si crea una tupla che verrà\n","# inserita nel set di chiavi\n","for _, row in redteam_df.iterrows():\n","    key = (\n","        row['time_rt'],\n","        row['src_user_rt'],\n","        row['src_comp_rt'],\n","        row['dst_comp_rt']\n","    )\n","    redteam_event_keys.add(key)\n","print(f\"Creato set con {len(redteam_event_keys)} chiavi di anomalie uniche.\")"],"metadata":{"id":"bhxLQsQgl_sb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Elaborazione per chunk (selezione anomalie e campionamento eventi normali)"],"metadata":{"id":"RsNUxxH-yE2h"}},{"cell_type":"code","source":["print(f\"Inizio elaborazione di {auth_file_path} in chunk...\")\n","print(f\"Dimensione del singolo chunk: {CHUNK_SIZE}\")\n","print(f\"Rapporto di campionamento delle righe normali: {NORMAL_ROWS_RATIO}\")\n","print(f\"Rapporto di campionamento delle righe normali per chunk senza anomalie: {NORMAL_ROWS_SAMPLING_RATE_FALLBACK}\")\n","\n","kept_anomalous_rows_list = [] # Lista delle righe anomale da conservare\n","kept_normal_rows_list = []    # Lista delle righe normali da conservare\n","total_rows_processed = 0      # Conteggio delle righe totali processate\n","\n","# Lettura di auth.txt.gz in chunk\n","auth_reader = pd.read_csv(\n","    auth_file_path,\n","    header=None,\n","    names=auth_cols,\n","    sep=',',\n","    compression='gzip',\n","    chunksize=CHUNK_SIZE\n",")\n","\n","chunk_num = 0\n","\n","for chunk_df in auth_reader:\n","    chunk_num += 1\n","    total_rows_processed += len(chunk_df)\n","    print(f\"\\nElaborazione del chunk {chunk_num}, ({len(chunk_df)} righe)\")\n","    print(f\"Righe totali processate: {total_rows_processed}\")\n","\n","    # Crea le chiavi per ogni riga nel chunk corrente nello stesso formato delle chiavi di redteam_event_keys\n","    # Per ogni riga applica la funzione tuple a questi quattro valori\n","    # L'intera serie viene poi assegnata alla variabile chunk_keys\n","    chunk_keys = chunk_df[['time', 'src_user', 'src_comp', 'dst_comp']].apply(tuple, axis=1)\n","\n","    # Identifica le righe anomale nel chunk\n","    is_anomaly_mask = chunk_keys.isin(redteam_event_keys)\n","\n","    # Il chunk corrente viene diviso in due DataFrame\n","    anomalous_in_chunk = chunk_df[is_anomaly_mask]\n","    normal_in_chunk = chunk_df[~is_anomaly_mask]\n","\n","    # Se il DataFrame anomalous_in_chunk non è vuoto\n","    # aggiunge queste righe alla lista kept_anomalous_rows_list\n","    if not anomalous_in_chunk.empty:\n","        kept_anomalous_rows_list.append(anomalous_in_chunk)\n","        print(f\"  Trovate e conservate {len(anomalous_in_chunk)} righe anomale.\")\n","\n","        num_anomalies_in_current_chunk = len(anomalous_in_chunk)\n","        # Logica di campionamento per le righe normali del chunk\n","        num_normal_to_sample_in_current_chunk = int(num_anomalies_in_current_chunk * NORMAL_ROWS_RATIO)\n","\n","        # Controllo per non campionare più righe normali di quante ce ne sono (non si verifica mai)\n","        num_normal_to_sample_in_current_chunk = min(num_normal_to_sample_in_current_chunk, len(normal_in_chunk))\n","\n","        print(f\"  Campionamento di {num_normal_to_sample_in_current_chunk} righe normali.\")\n","\n","    else: # Nessuna anomalia nel chunk\n","        if not normal_in_chunk.empty:\n","            # Logica di campionamento per le righe normali del chunk in caso di fallback\n","            num_normal_to_sample_in_current_chunk = int(len(normal_in_chunk) * NORMAL_ROWS_SAMPLING_RATE_FALLBACK)\n","            print(f\"  Nessuna anomalia trovata, Campionamento di {num_normal_to_sample_in_current_chunk} righe normali.\")\n","        else:\n","            num_normal_to_sample_in_current_chunk = 0\n","            print(f\"  Nessuna anomalia e nessuna riga normale in questo chunk.\")\n","\n","    # Procedi con il campionamento se num_normal_to_sample_in_current_chunk > 0\n","    if num_normal_to_sample_in_current_chunk > 0 and not normal_in_chunk.empty:\n","        sampled_normal_from_current_chunk = normal_in_chunk.sample(\n","            n=num_normal_to_sample_in_current_chunk,\n","            random_state=42 # Riproducibilità dei risultati\n","        )\n","        kept_normal_rows_list.append(sampled_normal_from_current_chunk)\n","        print(f\"  Campionate e conservate {len(sampled_normal_from_current_chunk)} righe normali.\")\n","    elif not normal_in_chunk.empty : # Se non si campiona nulla ma c'erano normali\n","        print(f\"  Non sono state campionate righe normali da questo chunk (calcolate {num_normal_to_sample_in_current_chunk}).\")\n","\n","print(\"\\nElaborazione di tutti i chunk completata.\")"],"metadata":{"id":"xJvd-4A6s__3","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creazione del DataFrame finale ridotto\n","\n"],"metadata":{"id":"vZD4eCh_yQv0"}},{"cell_type":"code","source":["# Creazione del DataFrame ridotto\n","print(\"Creazione del DataFrame ridotto auth_reduced_df...\")\n","auth_reduced_list_of_dfs = []\n","if kept_anomalous_rows_list:\n","    auth_reduced_list_of_dfs.extend(kept_anomalous_rows_list)\n","if kept_normal_rows_list:\n","    auth_reduced_list_of_dfs.extend(kept_normal_rows_list)\n","\n","# Concatenazione dei due DataFrame\n","auth_reduced_df = pd.concat(auth_reduced_list_of_dfs, ignore_index=True)\n","\n","# Ordinamento temporale del DataFrame ridotto\n","auth_reduced_df = auth_reduced_df.sort_values(by='time').reset_index(drop=True)\n","\n","print(f\"auth_reduced_df creato.\")\n","print(f\"Righe: {auth_reduced_df.shape[0]}\")\n","print(f\"Colonne: {auth_reduced_df.shape[1]}\")\n","\n","# Conteggio delle righe anomale e normali\n","if not auth_reduced_df.empty:\n","    print(\"\\nIspezione di auth_reduced_df:\")\n","    print(auth_reduced_df.head())\n","    auth_reduced_df.info()\n","\n","    # Conteggio approssimativo delle anomalie in auth_reduced_df\n","    num_anomalous_captured = sum(len(df) for df in kept_anomalous_rows_list)\n","    num_normal_sampled = sum(len(df) for df in kept_normal_rows_list)\n","\n","    print(f\"\\nNumero di righe anomale (basate su redteam) catturate in auth_reduced_df: {num_anomalous_captured}\")\n","    print(f\"Numero di righe normali campionate in auth_reduced_df: {num_normal_sampled}\")\n","    print(f\"Numero totale di chiavi anomale uniche in redteam: {len(redteam_event_keys)}\")\n","\n","    if num_anomalous_captured == len(redteam_event_keys):\n","        print(\"\\nTutte le anomalie uniche da redteam sono state trovate e conservate.\")\n","    elif num_anomalous_captured < len(redteam_event_keys):\n","        print(\"\\nAlcune anomalie da redteam potrebbero non essere state trovate in auth.txt.gz.\")\n","        print(\"Possibili corrispondenze inesatte o duplicati in redteam_df.\")\n","    elif num_anomalous_captured > len(redteam_event_keys):\n","        print(\"\\nCatturate più righe anomale rispetto alle chiavi uniche in redteam.\")\n","        print(\"Possibili eventi redteam corrispondenti a più righe in auth.txt.gz.\")\n","else:\n","    print(\"auth_reduced_df è vuoto.\")"],"metadata":{"id":"7L2qXI7Wjabf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Salvataggio del DataFrame finale ridotto"],"metadata":{"id":"DdB532W4yTQw"}},{"cell_type":"code","source":["print(f\"Salvataggio di auth_reduced_df...\")\n","\n","# Nome per il DataFrame ridotto\n","auth_reduced_csv_gz = f\"auth_reduced_{NORMAL_ROWS_RATIO}.csv.gz\"\n","auth_reduced_csv = f\"auth_reduced_{NORMAL_ROWS_RATIO}.csv\"\n","\n","# Percorso per il DataFrame ridotto\n","auth_reduced_path_csv_gz = f'{dataset_path}/dataset_reduced/{auth_reduced_csv_gz}'\n","auth_reduced_path_csv = f'{dataset_path}/dataset_reduced/{auth_reduced_csv}'\n","\n","try:\n","    # Salvataggio in formato CSV compresso\n","    auth_reduced_df.to_csv(auth_reduced_path_csv_gz,\n","                            index=False,\n","                            compression='gzip')\n","    print(f\"DataFrame ridotto salvato con successo come file CSV compresso in: {auth_reduced_path_csv_gz}\")\n","\n","    # Salvataggio in formato CSV\n","    auth_reduced_df.to_csv(auth_reduced_path_csv,\n","                            index=False)\n","    print(f\"DataFrame ridotto salvato con successo come file CSV in: {auth_reduced_path_csv}\")\n","\n","except Exception as e:\n","    print(f\"Si è verificato un errore durante il salvataggio di auth_reduced_df: {e}\")"],"metadata":{"id":"4haqZUDgwwkW"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1eGPs04RhYmPvxCEAWXKP0jtwFW_RdCYR","authorship_tag":"ABX9TyPeXfbQRBRgD7UANVykW3CG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}