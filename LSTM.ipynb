{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L86cTLtozwYe"
      },
      "outputs": [],
      "source": [
        "# Import necessari\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from sklearn.metrics import precision_recall_fscore_support, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj8V7qpCP24D"
      },
      "outputs": [],
      "source": [
        "# Variabili globali\n",
        "VERSION = \"v2\"\n",
        "WINDOW_SIZE = 60\n",
        "STRIDE = 1\n",
        "\n",
        "# Iperparametri del modello e di addestramento\n",
        "DROPOUT_RATE = 0.2              # Dropout rate\n",
        "LEARNING_RATE = 1e-4            # Learning rate\n",
        "WEIGHT_DECAY = 1e-4             # Regolarizzazione L2 per l'optimizer\n",
        "NUM_EPOCHS = 20                 # Epoche massime di addestramento\n",
        "EMBEDDING_DIM = 16              # Dimensione di default per gli embedding\n",
        "HIDDEN_DIM = 128                # Dimensione dello stato nascosto\n",
        "NUM_LAYERS = 1                  # LSTM Unidirezionale\n",
        "OUTPUT_DIM = 1                  # Dimensione dell'output del modello (binaria)\n",
        "USE_BATCH_NORM_EMB = True       # Applica BatchNorm dopo la concatenazione degli embedding\n",
        "USE_BATCH_NORM_LSTM_OUT = True  # Applica BatchNorm dopo l'output LSTM (prima dei linear heads)\n",
        "BATCH_SIZE = 64                 # Dimensione dei batch per il training e la valutazione\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_apUfpBOtQC",
        "outputId": "b72f76da-a5b2-4a7d-a1ca-ae2a085f3b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Montaggio Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Percorsi\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV'\n",
        "sequences_path = f'{base_path}/Sequences'\n",
        "embeddings_path = f'{base_path}/Embeddings'\n",
        "models_path = f'{base_path}/Models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8JCPdSWU2WG"
      },
      "outputs": [],
      "source": [
        "# Percorsi ai file .npy salvati\n",
        "sequences_folder_name = f\"sequences_{VERSION}_W{WINDOW_SIZE}_S{STRIDE}\"\n",
        "user_split_strategy_folder_name = f\"user_split_W{WINDOW_SIZE}_S{STRIDE}\"\n",
        "split_data_dir = f'{sequences_path}/{sequences_folder_name}/{user_split_strategy_folder_name}'\n",
        "\n",
        "X_train_path = f'{split_data_dir}/X_train_normal_only.npy'\n",
        "y_train_path = f'{split_data_dir}/y_train_normal_only.npy' # Valori 0\n",
        "X_val_path = f'{split_data_dir}/X_val_user_split.npy'\n",
        "y_val_path = f'{split_data_dir}/y_val_user_split.npy' # Valori 0 e 1\n",
        "X_test_path = f'{split_data_dir}/X_test_user_split.npy'\n",
        "y_test_path = f'{split_data_dir}/y_test_user_split.npy' # Valori 0 e 1\n",
        "\n",
        "# Caricamento degli array NumPy preprocessati\n",
        "print(f\"Caricamento X_train da: {X_train_path}\")\n",
        "X_train_np = np.load(X_train_path)\n",
        "print(f\"Caricamento y_train da: {y_train_path}\")\n",
        "y_train_np = np.load(y_train_path)\n",
        "\n",
        "print(f\"Caricamento X_val da: {X_val_path}\")\n",
        "X_val_np = np.load(X_val_path)\n",
        "print(f\"Caricamento y_val da: {y_val_path}\")\n",
        "y_val_np = np.load(y_val_path)\n",
        "\n",
        "print(f\"Caricamento X_test da: {X_test_path}\")\n",
        "X_test_np = np.load(X_test_path)\n",
        "print(f\"Caricamento y_test da: {y_test_path}\")\n",
        "y_test_np = np.load(y_test_path)\n",
        "\n",
        "# Stampa delle dimensioni dei dati caricati\n",
        "print(f\"\\nForme dei dati caricati:\")\n",
        "print(f\"X_train: {X_train_np.shape}, y_train: {y_train_np.shape}\")\n",
        "print(f\"X_val:   {X_val_np.shape},  y_val:   {y_val_np.shape}\")\n",
        "print(f\"X_test:  {X_test_np.shape}, y_test:  {y_test_np.shape}\")\n",
        "\n",
        "# File metadati embedding\n",
        "# Percorso al file JSON con i dati degli embedding (vocab sizes)\n",
        "embedding_data_path = f'{embeddings_path}/embedding_data_{VERSION}.json'\n",
        "\n",
        "# Carica embedding_data per ottenere le vocab_sizes\n",
        "with open(embedding_data_path, 'r') as f:\n",
        "    embedding_data_json = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyGVMmLS-oys"
      },
      "outputs": [],
      "source": [
        "# Determina le feature da usare per le sequenze\n",
        "# Lista delle feature usate per creare X_final_sequences\n",
        "# Questa lista DEVE corrispondere a 'feature_cols_for_sequences' nel codice di preprocessing\n",
        "categorical_feature_names = [\n",
        "    'src_user', 'dst_user', 'src_comp', 'dst_comp',\n",
        "    'auth_type', 'logon_type', 'auth_orientation', 'status'\n",
        "]\n",
        "\n",
        "# Nomi delle feature continue\n",
        "continuous_feature_names = ['time_scaled_per_user'] # Assumendo che sia l'ultima/e colonna/e\n",
        "\n",
        "# Determina il numero di feature categoriche e continue\n",
        "num_categorical_features = len(categorical_feature_names)\n",
        "num_continuous_features = len(continuous_feature_names)\n",
        "\n",
        "print(f\"Nomi feature categoriche (per embedding e vocab_size): {categorical_feature_names}\")\n",
        "print(f\"Nomi feature continue: {continuous_feature_names}\")\n",
        "\n",
        "# Verifica la coerenza con X_train_np.shape[2]\n",
        "# Il numero totale di feature deve corrispondere al numero di feature\n",
        "# effettivamente presenti nei dati caricati\n",
        "expected_total_features = num_categorical_features + num_continuous_features\n",
        "if X_train_np.shape[2] != expected_total_features:\n",
        "    print(f\"ATTENZIONE: Il numero totale di feature attese ({expected_total_features})\")\n",
        "    print(f\"non corrisponde alla terza dimensione di X_train ({X_train_np.shape[2]})!\")\n",
        "    print(\"Verifica l'ordine e i nomi in 'categorical_feature_names' e 'continuous_feature_names'.\")\n",
        "else:\n",
        "    print(f\"OK: Numero totale di feature ({expected_total_features}) corrisponde a X_train.shape[2].\")\n",
        "\n",
        "# Estrai le vocab_sizes solo per le feature categoriche\n",
        "# Crea la lista 'ordered_vocab_sizes' che verrà passata al modello\n",
        "ordered_vocab_sizes = []\n",
        "# Itera su 'categorical_feature_names'\n",
        "for original_col_name in categorical_feature_names:\n",
        "    vocab_size_key = f'{original_col_name}_vocab_size'\n",
        "    if vocab_size_key in embedding_data_json:\n",
        "        ordered_vocab_sizes.append(embedding_data_json[vocab_size_key])\n",
        "    else: # Fallback se la chiave non è trovata (improbabile)\n",
        "        vocab_size_key_alt = f'{original_col_name.replace(\"_encoded\", \"\")}_vocab_size'\n",
        "        if vocab_size_key_alt in embedding_data_json:\n",
        "            ordered_vocab_sizes.append(embedding_data_json[vocab_size_key_alt])\n",
        "        else:\n",
        "            raise ValueError(f\"Vocab size non trovata per {original_col_name} in {embedding_data_path} (chiavi provate: {vocab_size_key}, {vocab_size_key_alt})\")\n",
        "\n",
        "print(f\"\\nDimensioni dei vocabolari (solo per feature categoriche): {ordered_vocab_sizes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK1uYzGELpq4",
        "outputId": "4320cacf-3569-4fb3-e9b0-f47e836fd0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modello LSTM definito.\n"
          ]
        }
      ],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_sizes,              # Lista delle dimensioni del vocabolario (per ogni feature categorica)\n",
        "        embedding_dim,            # Dimensione dei vettori di embedding\n",
        "        num_continuous_features,  # Numero di feature continue da concatenare\n",
        "        hidden_dim,               # Numero di unità nel layer LSTM (dimensione dello stato nascosto)\n",
        "        output_dim,               # Dimensione dell'output (1 per la classificazione binaria)\n",
        "        num_layers,               # Numero di layer LSTM stacked\n",
        "        dropout_rate=0.2,         # Dropout rate\n",
        "        batch_first=True,         # Formato dell'input e dell'output dei tensori: (batch, seq, feature)\n",
        "        use_batch_norm_emb=False,\n",
        "        use_batch_norm_lstm_out=False\n",
        "        ):\n",
        "\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.num_categorical_features = len(vocab_sizes)\n",
        "        self.num_continuous_features = num_continuous_features\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.use_batch_norm_emb = use_batch_norm_emb\n",
        "        self.use_batch_norm_lstm_out = use_batch_norm_lstm_out\n",
        "\n",
        "        # Creazione lista di Embedding layers, uno per ogni feature categorica di input\n",
        "        # Ogni embedding layer ha la sua vocab_size specifica\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(num_embeddings=v_size, embedding_dim=embedding_dim) for v_size in vocab_sizes\n",
        "        ])\n",
        "\n",
        "        # Calcola la dimensione totale degli embedding concatenati\n",
        "        total_embedding_dim = self.num_categorical_features * embedding_dim\n",
        "\n",
        "        # L'input_size effettivo per l'LSTM sarà la somma delle dimensioni\n",
        "        # degli embedding e del numero di feature continue\n",
        "        lstm_input_size = total_embedding_dim + self.num_continuous_features\n",
        "\n",
        "        print(f\"LSTM input size calcolata: {lstm_input_size} (Embeddings: {total_embedding_dim} + Continue: {self.num_continuous_features})\")\n",
        "\n",
        "        # Definizione del layer BatchNorm1d\n",
        "        if self.use_batch_norm_emb and lstm_input_size  > 0 :\n",
        "             self.bn_emb_concat = nn.BatchNorm1d(lstm_input_size) # Normalizza sull'intera dimensione delle feature concatenate\n",
        "\n",
        "        # Definizione del layer LSTM\n",
        "        self.lstm = nn.LSTM(input_size=lstm_input_size,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=self.num_layers,\n",
        "                            dropout=dropout_rate , # if self.num_layers > 1 else 0,\n",
        "                            batch_first=batch_first)\n",
        "\n",
        "        # Definizione del layer BatchNorm1d da applicare all'output dell'LSTM\n",
        "        if self.use_batch_norm_lstm_out:\n",
        "            self.bn_lstm_out = nn.BatchNorm1d(hidden_dim) # Applicato all'output dell'LSTM\n",
        "\n",
        "        # Layer di Dropout separato, applicato dopo l'LSTM e BatchNorm\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # AI: Fully connected layer lineare per la classificazione finale\n",
        "        self.fc = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x, # L'input tensor (batch_size, seq_length, num_input_features) se batch_first=True\n",
        "        hidden_state=None # Opzionale per passare stati nascosti. Se None, viene inizializzato a zero\n",
        "        ):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Dividi l'input x in componenti categoriche e continue\n",
        "        # Assumiamo che le prime 'num_categorical_features' colonne siano categoriche\n",
        "        # e le successive 'num_continuous_features' siano continue.\n",
        "        x_categorical = x[:, :, :self.num_categorical_features].long() # .long() per i layer di embedding\n",
        "        x_continuous = x[:, :, self.num_categorical_features:] # Feature continue\n",
        "\n",
        "        # Processa ogni feature categorica attraverso il suo layer di embedding\n",
        "        embedded_features_list = []\n",
        "        for i in range(self.num_categorical_features):\n",
        "            feature_column = x_categorical[:, :, i]\n",
        "            embedded_feature = self.embeddings[i](feature_column)\n",
        "            embedded_features_list.append(embedded_feature)\n",
        "\n",
        "        # Concatena gli embedding risultanti\n",
        "        if self.num_categorical_features > 0:\n",
        "            combined_embeddings = torch.cat(embedded_features_list, dim=2) # (batch, seq, num_cat_feat * emb_dim)\n",
        "             # Concatena le feature continue (se presenti) agli embedding\n",
        "            if self.num_continuous_features > 0:\n",
        "                # Assicurati che x_continuous sia float\n",
        "                lstm_input_features = torch.cat((combined_embeddings, x_continuous.float()), dim=2)\n",
        "            else:\n",
        "                lstm_input_features = combined_embeddings\n",
        "        elif self.num_continuous_features > 0: # Solo feature continue\n",
        "            lstm_input_features = x_continuous.float()\n",
        "        else: # Nessuna feature, errore\n",
        "            raise ValueError(\"Il modello deve avere almeno una feature categorica o continua.\")\n",
        "\n",
        "        # Batch Normalization dopo la concatenazione degli embedding e delle feature concatenate\n",
        "        if hasattr(self, 'bn_emb_concat') and self.use_batch_norm_emb:\n",
        "            # BatchNorm1d si aspetta (N, C) o (N, C, L)\n",
        "            # Qui abbiamo (N, L, C). Occorre permutare\n",
        "            lstm_input_features = lstm_input_features.permute(0, 2, 1) # (batch, features, seq_len)\n",
        "            lstm_input_features = self.bn_emb_concat(lstm_input_features)\n",
        "            lstm_input_features = lstm_input_features.permute(0, 2, 1) # (batch, seq_len, features)\n",
        "\n",
        "        # Inizializzazione a zero degli stati nascosti (h0, c0) per l'LSTM se non forniti\n",
        "        if hidden_state is None:\n",
        "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "            hidden_state = (h0, c0)\n",
        "\n",
        "        # Passaggio attraverso il layer LSTM\n",
        "        # lstm_out contiene gli output dell'LSTM per ogni time step\n",
        "        # hidden_state contiene l'ultimo stato nascosto e di cella\n",
        "        lstm_out, hidden_state = self.lstm(lstm_input_features, hidden_state)\n",
        "\n",
        "        # Per la classificazione della sequenza si usa l'output dell'LSTM all'ultimo time step\n",
        "        last_time_step_out = lstm_out[:, -1, :] # (batch_size, hidden_dim)\n",
        "\n",
        "        # Applica BatchNorm1d opzionale all'output dell'ultimo time step dell'LSTM\n",
        "        if hasattr(self, 'bn_lstm_out') and self.use_batch_norm_lstm_out: # Aggiunto self.use_batch_norm_lstm_out\n",
        "            last_time_step_out = self.bn_lstm_out(last_time_step_out)\n",
        "\n",
        "        # Applica il Dropout\n",
        "        out = self.dropout(last_time_step_out)\n",
        "\n",
        "        # Passa attraverso il layer finale per ottenere il logit di output\n",
        "        out = self.fc(out) # (batch_size, output_dim)\n",
        "\n",
        "        # Restituisce i logits e l'ultimo stato nascosto\n",
        "        return out, hidden_state\n",
        "\n",
        "    # Metodo helper per inizializzare gli stati nascosti (non usato esplicitamente\n",
        "    # perché l'LSTM li inizializza a zero di default se hidden_state=None)\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                  weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden\n",
        "\n",
        "print(\"Modello LSTM definito.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luL1TFeVOuaJ"
      },
      "outputs": [],
      "source": [
        "print(f\"Dimensioni dei vocabolari (nell'ordine delle feature): {ordered_vocab_sizes}\")\n",
        "\n",
        "# Istanzia il modello\n",
        "model = LSTM(\n",
        "    vocab_sizes=ordered_vocab_sizes,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    num_continuous_features=num_continuous_features,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    output_dim=OUTPUT_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout_rate=DROPOUT_RATE,\n",
        "    use_batch_norm_emb=USE_BATCH_NORM_EMB,\n",
        "    use_batch_norm_lstm_out=USE_BATCH_NORM_LSTM_OUT\n",
        "    )\n",
        "\n",
        "print(\"\\nModello LSTM istanziato:\")\n",
        "print(model) # Stampa la struttura del modello\n",
        "\n",
        "# Sposta il modello sulla GPU se disponibile\n",
        "model.to(device)\n",
        "print(f\"Modello spostato su: {device}\")\n",
        "\n",
        "# Creazione DataLoaders\n",
        "# Conversione degli array NumPy in Tensori PyTorch\n",
        "# Gli input X devono essere LongTensor per i layer di embedding\n",
        "X_train_tensor = torch.from_numpy(X_train_np).float()\n",
        "y_train_tensor = torch.from_numpy(y_train_np).float() # Per BCEWithLogitsLoss\n",
        "\n",
        "X_val_tensor = torch.from_numpy(X_val_np).float()\n",
        "y_val_tensor = torch.from_numpy(y_val_np).float()\n",
        "\n",
        "X_test_tensor = torch.from_numpy(X_test_np).float()\n",
        "y_test_tensor = torch.from_numpy(y_test_np).float()\n",
        "\n",
        "# Creazione di TensorDataset, che incapsula tensori con la stessa prima dimensione\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Creazione dei DataLoader, che gestiscono il batching, lo shuffle e il caricamento parallelo dei dati\n",
        "# shuffle=True per il train_loader per ridurre la varianza del gradiente e migliorare la generalizzazione\n",
        "# drop_last=True per il train_loader per evitare batch di dimensioni diverse che potrebbero dare problemi con alcuni layer\n",
        "# shuffle=False per val_loader e test_loader perché l'ordine non deve cambiare per una valutazione consistente\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) # Shuffle per il training\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"\\nDataLoaders creati.\")\n",
        "print(f\"Numero di batch in train_loader: {len(train_loader)}\")\n",
        "print(f\"Numero di batch in val_loader: {len(val_loader)}\")\n",
        "\n",
        "# Esempio di un batch dal train_loader\n",
        "# Utile per verificare le dimensioni dei tensori prodotti dal DataLoader\n",
        "if len(train_loader) > 0: # Controllo per evitare errori se il loader è vuoto\n",
        "    dataiter = iter(train_loader)\n",
        "    sample_x, sample_y = next(dataiter)\n",
        "    print(f\"\\nForma di un batch di input X: {sample_x.shape}\") # (BATCH_SIZE, WINDOW_SIZE, num_total_features=9)\n",
        "    print(f\"Forma di un batch di output y: {sample_y.shape}\") # (BATCH_SIZE)\n",
        "else:\n",
        "    print(\"Train loader è vuoto.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eNYC_xhyOyDQ"
      },
      "outputs": [],
      "source": [
        "# Per classificazione binaria con logits in output\n",
        "# Il modello impara cosa è \"normale\" cercando di predire 0 (o un logit molto negativo)\n",
        "# Le anomalie (non viste in training) dovrebbero produrre logits meno negativi o positivi\n",
        "criterion = nn.BCEWithLogitsLoss() # Combina la sigmoide e la BCE\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "print(\"Loss function e optimizer definiti.\")\n",
        "\n",
        "# Setup per l'Early Stopping\n",
        "best_val_loss = float('inf') # Inizializza la miglior loss di validazione a infinito\n",
        "epochs_no_improve = 0 # Contatore per le epoche senza miglioramento\n",
        "patience = 2 # Numero di epoche da attendere prima di fermare il training se la Val Loss non migliora\n",
        "best_model_weights_path = f\"{models_path}/lstm_best_model_weights.pt\"\n",
        "\n",
        "print(f\"\\nInizio training per {NUM_EPOCHS} epoche...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train() # Mette il modello in modalità training\n",
        "    train_loss_accum = 0\n",
        "\n",
        "    # Itera sui batch del training set\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Azzera i gradienti accumulati dal batch precedente\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: ottiene i logits dal modello\n",
        "        # Il secondo output (stato nascosto) dell'LSTM non è usato qui\n",
        "        outputs, _ = model(inputs)\n",
        "\n",
        "        # Calcola la loss\n",
        "        # .squeeze() rimuove la dimensione superflua (es. da [64,1] a [64])\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "        # Calcola i gradienti della loss rispetto ai parametri del modello (backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # Aggiorna i pesi del modello usando i gradienti calcolati\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumula la loss del batch\n",
        "        train_loss_accum += loss.item()\n",
        "\n",
        "    # Calcolo loss media di training per l'epoca\n",
        "    avg_train_loss = train_loss_accum / len(train_loader)\n",
        "\n",
        "    # VALIDATION LOOP\n",
        "    model.eval() # Mette il modello in modalità valutazione\n",
        "    val_loss_accum = 0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "\n",
        "    with torch.no_grad(): # Disabilita il calcolo dei gradienti durante la validazione\n",
        "        for inputs_val, labels_val in val_loader:\n",
        "            inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)\n",
        "\n",
        "            outputs_val, _ = model(inputs_val)\n",
        "\n",
        "            val_loss = criterion(outputs_val.squeeze(), labels_val)\n",
        "            val_loss_accum += val_loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss_accum / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Logica di Early Stopping\n",
        "    # Se la Val Loss attuale è migliore della migliore finora\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        # Salva i pesi del modello\n",
        "        torch.save(model.state_dict(), best_model_weights_path)\n",
        "        print(f\"  Miglioramento Val Loss: {best_val_loss:.4f}. Modello salvato in {best_model_weights_path}\")\n",
        "        # Resetta il contatore delle epoche senza miglioramento\n",
        "        epochs_no_improve = 0\n",
        "    else: # Se la Val Loss non è migliorata\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"  Nessun miglioramento Val Loss per {epochs_no_improve} epoche.\")\n",
        "    # Se la Val Loss non migliora per 'patience' epoche consecutive\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"Early stopping attivato dopo {epoch+1} epoche.\")\n",
        "        break # Interrompi il training\n",
        "\n",
        "print(\"\\nTraining completato.\")\n",
        "\n",
        "print(f\"Caricamento del modello con la migliore Val Loss da: {best_model_weights_path}\")\n",
        "model.load_state_dict(torch.load(best_model_weights_path))\n",
        "print(\"Modello migliore caricato.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rpwCuPBg0h0N",
        "outputId": "582f746e-027a-4e86-d738-c68308b28f98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Funzione get_predictions_and_labels definita.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_predictions_and_labels(model, data_loader, device_eval):\n",
        "    model.eval()  # Mette il modello in modalità valutazione\n",
        "    all_labels_eval = []\n",
        "    all_probs_eval = []\n",
        "\n",
        "    with torch.no_grad():  # Disabilita il calcolo dei gradienti\n",
        "        for inputs_eval, labels_eval in data_loader:\n",
        "            inputs_eval = inputs_eval.to(device_eval)\n",
        "\n",
        "            # Ottieni l'output del modello\n",
        "            model_output = model(inputs_eval)\n",
        "\n",
        "            # Gestisce il caso in cui il modello restituisca una tupla (es. logits, attention_weights)\n",
        "            # o solo i logits (per il modello LSTM attuale, model_output è già una tupla) (outputs, hidden_state)\n",
        "            if isinstance(model_output, tuple):\n",
        "                outputs_logits_eval = model_output[0] # Prende il primo elemento (i logits)\n",
        "            else: # Il modello restituisce solo un tensore (i logits)\n",
        "                outputs_logits_eval = model_output\n",
        "\n",
        "            # Rimuove dimensioni superflue\n",
        "            squeezed_logits = outputs_logits_eval.squeeze()\n",
        "            # Converte i logits in probabilità\n",
        "            probs_for_batch = torch.sigmoid(squeezed_logits)\n",
        "\n",
        "            # Raccoglie le etichette vere\n",
        "            all_labels_eval.extend(labels_eval.cpu().numpy().tolist())\n",
        "\n",
        "            # Converti le probabilità in NumPy e gestisci il caso 0-D\n",
        "            np_probs_for_batch = probs_for_batch.cpu().numpy()\n",
        "\n",
        "            # Caso in cui probs_for_batch è uno scalare (se batch_size=1 e squeeze() rimuove tutte le dim)\n",
        "            if np_probs_for_batch.ndim == 0:  # È uno scalare (array 0-D)\n",
        "                all_probs_eval.append(float(np_probs_for_batch))\n",
        "            else:\n",
        "                all_probs_eval.extend(np_probs_for_batch.tolist())\n",
        "    return np.array(all_labels_eval), np.array(all_probs_eval)\n",
        "\n",
        "print(\"Funzione get_predictions_and_labels definita.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FxcFtkBVWWcv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Ottieni predizioni una sola volta\n",
        "print(\"Ottenimento predizioni finali dal modello migliore...\")\n",
        "# Carica il modello migliore e ottiene le predizioni (probabilità)\n",
        "# e le etichette vere per il validation e test set\n",
        "val_labels_final, val_probs_final = get_predictions_and_labels(model, val_loader, device)\n",
        "test_labels_final, test_probs_final = get_predictions_and_labels(model, test_loader, device)\n",
        "print(\"Predizioni finali ottenute.\")\n",
        "\n",
        "# Visualizzazione della distribuzione delle probabilità (VALIDATION SET)\n",
        "# Questa sezione serve a capire come il modello distribuisce le probabilità\n",
        "# per le classi normali e anomale sul validation set. Serve una buona separazione\n",
        "print(\"\\nDistribuzione delle probabilità predette sul Validation Set:\")\n",
        "val_probs_normal = val_probs_final[val_labels_final == 0]\n",
        "val_probs_anomalous = val_probs_final[val_labels_final == 1]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(val_probs_normal, bins=50, alpha=0.7, label=f'Normali (Validation) (n={len(val_probs_normal)})', color='blue', density=True)\n",
        "if len(val_probs_anomalous) > 0:\n",
        "    plt.hist(val_probs_anomalous, bins=50, alpha=0.7, label=f'Anomale (Validation) (n={len(val_probs_anomalous)})', color='red', density=True)\n",
        "else:\n",
        "    print(\"Nessuna anomalia nel validation set per plottare il suo istogramma.\")\n",
        "plt.title('Distribuzione delle Probabilità Predette sul Validation Set (dal Modello Migliore)')\n",
        "plt.xlabel('Probabilità Predetta di Anomalia')\n",
        "plt.ylabel('Densità')\n",
        "plt.legend()\n",
        "plt.yscale('log') # Utile se le probabilità sono molto piccole\n",
        "plt.show()\n",
        "\n",
        "# Stampa statistiche descrittive delle probabilità per le due classi\n",
        "print(f\"Statistiche val_probs_normal: N={len(val_probs_normal)}, Min={np.min(val_probs_normal):.2e}, Max={np.max(val_probs_normal):.2e}, Mean={np.mean(val_probs_normal):.2e}, Median={np.median(val_probs_normal):.2e}\")\n",
        "if len(val_probs_anomalous) > 0:\n",
        "    print(f\"Statistiche val_probs_anomalous: N={len(val_probs_anomalous)}, Min={np.min(val_probs_anomalous):.2e}, Max={np.max(val_probs_anomalous):.2e}, Mean={np.mean(val_probs_anomalous):.2e}, Median={np.median(val_probs_anomalous):.2e}\")\n",
        "\n",
        "# Determinazione della soglia ottimale con il metodo del percentile (VALIDATION SET)\n",
        "print(\"\\n--- Determinazione Soglia Ottimale con Metodo Percentile (su Validation Set) ---\")\n",
        "percentiles_to_try = [85, 90, 95, 97, 98, 99, 99.5, 99.9]\n",
        "best_f1_val_percentile = -1 # Inizializza a -1.0 per assicurare che qualsiasi F1 valido sia maggiore\n",
        "final_optimal_threshold = 0.5 # Soglia di default se non si trovano anomalie\n",
        "best_percentile_chosen = 0\n",
        "num_anomalies_val = np.sum(val_labels_final == 1)\n",
        "print(f\"Numero di anomalie effettive nel Validation Set: {num_anomalies_val}\")\n",
        "\n",
        "# Cerca la soglia percentile che massimizza l'F1-score sulla classe anomala\n",
        "if len(val_probs_final) > 0 and num_anomalies_val > 0:\n",
        "    for p_val in percentiles_to_try:\n",
        "        # Calcola la soglia per il percentile corrente\n",
        "        current_threshold = np.percentile(val_probs_final, p_val)\n",
        "        # Classifica in base alla soglia\n",
        "        val_preds_p = (val_probs_final >= current_threshold).astype(int)\n",
        "\n",
        "        precision_p = precision_score(val_labels_final, val_preds_p, pos_label=1, zero_division=0)\n",
        "        recall_p = recall_score(val_labels_final, val_preds_p, pos_label=1, zero_division=0)\n",
        "        f1_p = f1_score(val_labels_final, val_preds_p, pos_label=1, zero_division=0)\n",
        "\n",
        "        print(f\"  Percentile: {p_val:>5}% -> Soglia: {current_threshold:.2e} | P: {precision_p:.4f}, R: {recall_p:.4f}, F1: {f1_p:.4f}\")\n",
        "\n",
        "        # Ottimizza per F1-score\n",
        "        if f1_p > best_f1_val_percentile:\n",
        "            best_f1_val_percentile = f1_p\n",
        "            final_optimal_threshold = current_threshold\n",
        "            best_percentile_chosen = p_val\n",
        "    print(f\"\\nMiglior Soglia (Percentile) scelta: {final_optimal_threshold:.2e} (dal {best_percentile_chosen}° percentile, F1 su Val: {best_f1_val_percentile:.4f})\")\n",
        "elif num_anomalies_val == 0:\n",
        "    print(\"Nessuna anomalia nel validation set, non posso ottimizzare la soglia percentile in modo significativo. Uso default 0.5.\")\n",
        "else: # val_probs_final è vuoto\n",
        "    print(\"val_probs_final è vuoto, non posso calcolare la soglia. Uso default 0.5.\")\n",
        "\n",
        "# Valutazione sul validation set con la soglia scelta\n",
        "print(f\"\\n--- Performance sul Validation Set con Soglia Finale ({final_optimal_threshold:.2e}) ---\")\n",
        "val_predicted_labels_final = (val_probs_final >= final_optimal_threshold).astype(int)\n",
        "print(classification_report(val_labels_final, val_predicted_labels_final, target_names=['Normale (0)', 'Anomalo (1)'], zero_division=0))\n",
        "cm_val = confusion_matrix(val_labels_final, val_predicted_labels_final)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm_val, annot=True, fmt='d', cmap='BuGn', xticklabels=['Pred. Normale', 'Pred. Anomalo'], yticklabels=['Vero Normale', 'Vero Anomalo'])\n",
        "plt.title(f'Validation Set - Confusion Matrix (Soglia={final_optimal_threshold:.2e})')\n",
        "plt.xlabel('Etichetta Predetta'); plt.ylabel('Etichetta Vera'); plt.show()\n",
        "\n",
        "# Valutazione finale sul test set con la soglia scelta\n",
        "print(f\"\\n--- Performance Finale sul Test Set con Soglia ({final_optimal_threshold:.2e}) ---\")\n",
        "test_predicted_labels_final = (test_probs_final >= final_optimal_threshold).astype(int)\n",
        "\n",
        "print(\"\\nClassification Report completo sul Test Set:\")\n",
        "print(classification_report(test_labels_final, test_predicted_labels_final, target_names=['Normale (0)', 'Anomalo (1)'], zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix sul Test Set:\")\n",
        "cm_test = confusion_matrix(test_labels_final, test_predicted_labels_final)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='BuGn', xticklabels=['Pred. Normale', 'Pred. Anomalo'], yticklabels=['Vero Normale', 'Vero Anomalo'])\n",
        "plt.title(f'Test Set - Confusion Matrix (Soglia={final_optimal_threshold:.2e})')\n",
        "plt.xlabel('Etichetta Predetta'); plt.ylabel('Etichetta Vera'); plt.show()\n",
        "\n",
        "# Conteggio finale dei veri positivi sul test set\n",
        "num_anomalies_test = np.sum(test_labels_final == 1)\n",
        "true_positives_test = cm_test[1, 1] if num_anomalies_test > 0 and cm_test.shape == (2,2) else 0\n",
        "print(f\"Numero di anomalie effettive nel Test Set: {num_anomalies_test}\")\n",
        "if num_anomalies_test > 0:\n",
        "    print(f\"Anomalie correttamente identificate (TP) nel Test Set: {true_positives_test} su {num_anomalies_test}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}