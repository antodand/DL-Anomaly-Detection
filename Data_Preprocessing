{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCqQdRyA7lM/tw1IWAxlQ2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"I-d3XlLR3Fia"}},{"cell_type":"markdown","source":["## Setup iniziale"],"metadata":{"id":"ZqMv2jJgRitv"}},{"cell_type":"code","source":["VERSION = \"v2\"    # Versioning\n","WINDOW_SIZE = 30  # Dimensione sliding window\n","STRIDE = 5        # Di quanti eventi si sposta la sliding window (1 = massima sovrapposizione)"],"metadata":{"id":"7x8en_8rRy3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import json\n","from google.colab import drive\n","\n","# Montaggio Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Definizione percorsi\n","base_path = '/content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV'\n","dataset_path = f'{base_path}/Dataset'\n","embeddings_path = f'{base_path}/Embeddings'\n","sequences_path = f'{base_path}/Sequences'\n","auth_reduced_path = f'{dataset_path}/dataset_reduced/auth_reduced_1000.csv.gz'\n","redteam_file_path = f'{dataset_path}/redteam.txt.gz'\n","\n","# Definizione feature\n","# Colonne di auth (9)\n","auth_cols = [\n","    'time',\n","    'src_user',\n","    'dst_user',\n","    'src_comp',\n","    'dst_comp',\n","    'auth_type',\n","    'logon_type',\n","    'auth_orientation',\n","    'status'\n","]\n","# Colonne di redteam (4)\n","redteam_cols = [\n","    'time_rt',\n","    'src_user_rt',\n","    'src_comp_rt',\n","    'dst_comp_rt'\n","]"],"metadata":{"id":"D45rS0ph520c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593557600,"user_tz":-120,"elapsed":42452,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"d1389d9a-441c-49b3-ba10-108061328279"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Feature Engineering, Encoding e Creazione della colonna is_anomaly"],"metadata":{"id":"nayCdOsw3IIW"}},{"cell_type":"markdown","source":["### Aggiunta della colonna `is_anomaly`"],"metadata":{"id":"61v3ZcJE8ky7"}},{"cell_type":"code","source":["print(f\"Caricamento di auth ridotto da: {auth_reduced_path}\")\n","auth_reduced_df = pd.read_csv(\n","    auth_reduced_path,\n","    na_values='?',\n","    compression='gzip'\n","    )\n","print(f\"Caricato auth_reduced_df.\")\n","print(f\"Righe: {auth_reduced_df.shape[0]}\")\n","print(f\"Colonne: {auth_reduced_df.shape[1]}\")\n","\n","print(f\"\\nCaricamento di redteam da: {redteam_file_path}\")\n","redteam_df = pd.read_csv(\n","    redteam_file_path,\n","    header=None,\n","    names=redteam_cols,\n","    sep=',',\n","    compression='gzip'\n","    )\n","print(f\"Caricato redteam_df.\")\n","print(f\"Righe: {redteam_df.shape[0]}\")\n","print(f\"Colonne: {redteam_df.shape[1]}\")\n","\n","print(\"\\nCreazione del set di chiavi anomale da redteam_df...\")\n","redteam_event_keys = set()\n","for _, row in redteam_df.iterrows():\n","    key = (row['time_rt'], row['src_user_rt'], row['src_comp_rt'], row['dst_comp_rt'])\n","    redteam_event_keys.add(key)\n","print(f\"Creato set con {len(redteam_event_keys)} chiavi anomale uniche.\")\n","del redteam_df # Libera memoria\n","\n","print(\"Aggiunta della colonna 'is_anomaly' ad auth_reduced_df...\")\n","# Per ogni riga in auth_reduced_df, si crea una tupla chiave\n","reduced_df_keys = auth_reduced_df[['time', 'src_user', 'src_comp', 'dst_comp']].apply(tuple, axis=1)\n","# Si controlla se questa chiave è presente nel set redteam_event_keys\n","# Il risultato booleano viene convertito in intero\n","auth_reduced_df['is_anomaly'] = reduced_df_keys.isin(redteam_event_keys).astype(int)\n","del reduced_df_keys # Libera memoria\n","\n","# Forza la garbage collection per liberare ulteriore memoria\n","import gc\n","gc.collect()\n","\n","print(\"Colonna 'is_anomaly' aggiunta con successo.\")\n","print(f\"Conteggio valori {auth_reduced_df['is_anomaly'].value_counts(dropna=False)}\")"],"metadata":{"id":"GUXMuGKO7AQ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593564971,"user_tz":-120,"elapsed":7379,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"c6c501b1-7af5-4ce9-d911-554faf02c00d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Caricamento di auth ridotto da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Dataset/dataset_reduced/auth_reduced_1000.csv.gz\n","Caricato auth_reduced_df.\n","Righe: 780845\n","Colonne: 9\n","\n","Caricamento di redteam da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Dataset/redteam.txt.gz\n","Caricato redteam_df.\n","Righe: 749\n","Colonne: 4\n","\n","Creazione del set di chiavi anomale da redteam_df...\n","Creato set con 715 chiavi anomale uniche.\n","Aggiunta della colonna 'is_anomaly' ad auth_reduced_df...\n","Colonna 'is_anomaly' aggiunta con successo.\n","Conteggio valori is_anomaly\n","0    780143\n","1       702\n","Name: count, dtype: int64\n"]}]},{"cell_type":"markdown","source":["### Scaling della feature `time` per singolo utente con `StandardScaler`"],"metadata":{"id":"2V1D5HZRzzPq"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","# Inizializza la nuova colonna nel DataFrame\n","auth_reduced_df['time_scaled_per_user'] = np.nan\n","\n","# Raggruppa per 'src_user' e applica lo scaling\n","for user_name, group_df in auth_reduced_df.groupby('src_user'):\n","    if not group_df.empty:\n","        scaler = StandardScaler()\n","        # Reshape della colonna 'time' per renderla 2D, come richiesto da StandardScaler\n","        time_values_reshaped = group_df['time'].values.reshape(-1, 1)\n","\n","        # Applica fit_transform per calcolare media/std del gruppo e scalarlo\n","        scaled_time_values = scaler.fit_transform(time_values_reshaped)\n","\n","        # Assegna i valori scalati kembali al DataFrame originale usando .loc e l'indice del gruppo\n","        auth_reduced_df.loc[group_df.index, 'time_scaled_per_user'] = scaled_time_values.flatten()\n","\n","# Gestione per eventuali NaN rimasti\n","# StandardScaler dovrebbe già gestire gruppi con varianza zero (es. singolo evento) restituendo 0.0\n","auth_reduced_df['time_scaled_per_user'] = auth_reduced_df['time_scaled_per_user'].fillna(0.0)\n","\n","print(\"Scaling della feature 'time' per utente con StandardScaler completato.\")\n","print(f\"Descrizione di 'time_scaled_per_user':\\n{auth_reduced_df['time_scaled_per_user'].describe()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9xcViFq38vE","executionInfo":{"status":"ok","timestamp":1748593587717,"user_tz":-120,"elapsed":22743,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"8ad19f1f-39a7-4535-f93b-ee5f938703b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scaling della feature 'time' per utente con StandardScaler completato.\n","Descrizione di 'time_scaled_per_user':\n","count    7.808450e+05\n","mean     6.522639e-17\n","std      9.973981e-01\n","min     -6.846827e+00\n","25%     -5.425958e-01\n","50%     -2.194943e-01\n","75%      9.150619e-02\n","max      2.098161e+01\n","Name: time_scaled_per_user, dtype: float64\n"]}]},{"cell_type":"markdown","source":["### Imputazione valori `NaN` ed Encoding Feature Categoriche:"],"metadata":{"id":"ZWDwC4f_7492"}},{"cell_type":"code","source":["# Colonne categoriche che verranno usate per gli embedding\n","categorical_cols_to_embed = [\n","    'src_user',\n","    'dst_user',\n","    'src_comp',\n","    'dst_comp',\n","    'auth_type',\n","    'logon_type',\n","    'auth_orientation',\n","    'status'\n","]\n","\n","# Dizionario per memorizzare i vocabolari\n","feature_vocabs = {}\n","# Dizionario con i metadati per i layer di embedding\n","# (come la dimensione del vocabolario)\n","embedding_data = {}\n","# Placeholder per i valori mancanti nelle feature categoriche.\n","missing_value_placeholder = \"_MISSING_\"\n","\n","for col in categorical_cols_to_embed:\n","    print(f\"\\nProcessando colonna: {col}...\")\n","\n","    # Normalizzazione dei valori troncati specifica per 'auth_type'\n","    # Converte tutti i valori che iniziano con \"MICROSOFT_AUTHENTICA\"\n","    # al valore standard \"MICROSOFT_AUTHENTICATION_PACKAGE_V1_0\"\n","    if col == 'auth_type':\n","        print(f\"  Normalizzazione dei valori troncati della colonna '{col}'...\")\n","        condition = auth_reduced_df[col].astype(str).str.upper().str.startswith(\"MICROSOFT_AUTHENTICA\")\n","        auth_reduced_df.loc[condition, col] = \"MICROSOFT_AUTHENTICATION_PACKAGE_V1_0\"\n","        print(f\"  Normalizzazione per i valori troncati della colonna '{col}' verso MICROSOFT_AUTHENTICATION_PACKAGE_V1_0 completata.\")\n","\n","    # Conta i valori NaN nella colonna corrente\n","    nan_count = auth_reduced_df[col].isnull().sum()\n","\n","    # Se ci sono NaN, riempie con il placeholder\n","    # In questo modo i NaN siano trattati come una categoria specifica\n","    if nan_count > 0:\n","        print(f\"  Trovati {nan_count} valori NaN.\")\n","        print(f\"  Imputazione con '{missing_value_placeholder}'.\")\n","        auth_reduced_df[col] = auth_reduced_df[col].fillna(missing_value_placeholder)\n","    else:\n","        print(f\"  Nessun valore NaN trovato in questa colonna.\")\n","\n","    # Conversione delle feature da stringhe a categorie\n","    auth_reduced_df[col] = auth_reduced_df[col].astype('category')\n","    # Ottiene le categorie uniche (incluso `_MISSING_`)\n","    current_categories = auth_reduced_df[col].cat.categories\n","\n","    # Crea un dizionario per la feature corrente\n","    # Ogni categoria unica viene mappata ad un intero\n","    vocab = {cat: i for i, cat in enumerate(current_categories)}\n","    feature_vocabs[col] = vocab\n","\n","    # Calcola la dimensione del vocabolario per il layer di Embedding\n","    vocab_size = len(current_categories) # Numero esatto di categorie uniche\n","    embedding_data[f'{col}_vocab_size'] = vocab_size\n","\n","    # Crea una nuova colonna con i codici numerici per le categorie\n","    auth_reduced_df[f'{col}_encoded'] = auth_reduced_df[col].cat.codes\n","\n","    print(f\"    Dizionario creato con {len(vocab)} elementi (mappati da 1 a {len(vocab)}).\")\n","    print(f\"    Dimensione per Embedding Layer ('input_dim'): {vocab_size}.\")\n","    print(f\"    Colonna '{col}_encoded' aggiunta.\")\n","\n","print(\"\\nImputazione NaN ed encoding delle feature completati.\")"],"metadata":{"id":"IIo1BMII8VVS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593588787,"user_tz":-120,"elapsed":1075,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"48c9395d-70e0-4f5a-e778-9f09d0165ade"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Processando colonna: src_user...\n","  Nessun valore NaN trovato in questa colonna.\n","    Dizionario creato con 27767 elementi (mappati da 1 a 27767).\n","    Dimensione per Embedding Layer ('input_dim'): 27767.\n","    Colonna 'src_user_encoded' aggiunta.\n","\n","Processando colonna: dst_user...\n","  Nessun valore NaN trovato in questa colonna.\n","    Dizionario creato con 29962 elementi (mappati da 1 a 29962).\n","    Dimensione per Embedding Layer ('input_dim'): 29962.\n","    Colonna 'dst_user_encoded' aggiunta.\n","\n","Processando colonna: src_comp...\n","  Nessun valore NaN trovato in questa colonna.\n","    Dizionario creato con 13078 elementi (mappati da 1 a 13078).\n","    Dimensione per Embedding Layer ('input_dim'): 13078.\n","    Colonna 'src_comp_encoded' aggiunta.\n","\n","Processando colonna: dst_comp...\n","  Nessun valore NaN trovato in questa colonna.\n","    Dizionario creato con 11642 elementi (mappati da 1 a 11642).\n","    Dimensione per Embedding Layer ('input_dim'): 11642.\n","    Colonna 'dst_comp_encoded' aggiunta.\n","\n","Processando colonna: auth_type...\n","  Normalizzazione dei valori troncati della colonna 'auth_type'...\n","  Normalizzazione per i valori troncati della colonna 'auth_type' verso MICROSOFT_AUTHENTICATION_PACKAGE_V1_0 completata.\n","  Trovati 448593 valori NaN.\n","  Imputazione con '_MISSING_'.\n","    Dizionario creato con 6 elementi (mappati da 1 a 6).\n","    Dimensione per Embedding Layer ('input_dim'): 6.\n","    Colonna 'auth_type_encoded' aggiunta.\n","\n","Processando colonna: logon_type...\n","  Trovati 116751 valori NaN.\n","  Imputazione con '_MISSING_'.\n","    Dizionario creato con 10 elementi (mappati da 1 a 10).\n","    Dimensione per Embedding Layer ('input_dim'): 10.\n","    Colonna 'logon_type_encoded' aggiunta.\n","\n","Processando colonna: auth_orientation...\n","  Nessun valore NaN trovato in questa colonna.\n","    Dizionario creato con 7 elementi (mappati da 1 a 7).\n","    Dimensione per Embedding Layer ('input_dim'): 7.\n","    Colonna 'auth_orientation_encoded' aggiunta.\n","\n","Processando colonna: status...\n","  Nessun valore NaN trovato in questa colonna.\n","    Dizionario creato con 2 elementi (mappati da 1 a 2).\n","    Dimensione per Embedding Layer ('input_dim'): 2.\n","    Colonna 'status_encoded' aggiunta.\n","\n","Imputazione NaN ed encoding delle feature completati.\n"]}]},{"cell_type":"markdown","source":["## Salvataggio dizionari"],"metadata":{"id":"HnvP66A0gYop"}},{"cell_type":"code","source":["# Percorsi per salvare i dizionari dei vocabolari e dei metadati degli embedding\n","feature_vocabs_path = f'{embeddings_path}/feature_vocabs_{VERSION}.json'\n","embedding_data_path = f'{embeddings_path}/embedding_data_{VERSION}.json'\n","\n","# Salvataggio dei dizionari in formato JSON\n","print(\"\\nSalvataggio di feature_vocabs e embedding_data...\")\n","with open(feature_vocabs_path, 'w') as f:\n","    json.dump(feature_vocabs, f, indent=4)\n","print(f\"  feature_vocabs salvato in: {feature_vocabs_path}\")\n","with open(embedding_data_path, 'w') as f:\n","    json.dump(embedding_data, f, indent=4)\n","print(f\"  embedding_data salvato in: {embedding_data_path}\")\n","\n","print(\"\\nSalvataggio dei dizionari completato.\")"],"metadata":{"id":"9sAFm1pv3v4z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593590463,"user_tz":-120,"elapsed":1687,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"ddba74b5-f5eb-4411-c4a7-6c606e0dc937"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Salvataggio di feature_vocabs e embedding_data...\n","  feature_vocabs salvato in: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Embeddings/feature_vocabs_v2.json\n","  embedding_data salvato in: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Embeddings/embedding_data_v2.json\n","\n","Salvataggio dei dizionari completato.\n"]}]},{"cell_type":"markdown","source":["## Creazione e salvataggio del DataFrame finale per sequenziamento"],"metadata":{"id":"MK1z-41A8a7E"}},{"cell_type":"code","source":["import os\n","\n","# Crea una lista di tutti i nomi di colonna che terminano con \"_encoded\"\n","# e che sono effettivamente contenuti in auth_reduced_df\n","all_encoded_cols = [f'{col}_encoded' for col in categorical_cols_to_embed if f'{col}_encoded' in auth_reduced_df.columns]\n","\n","# Costruisce la lista finale di colonne (ordinate)\n","cols_for_final_df = ['time', 'time_scaled_per_user', 'src_user_encoded'] + \\\n","                      [enc_col for enc_col in all_encoded_cols if enc_col != 'src_user_encoded'] + \\\n","                      ['is_anomaly']\n","# Controllo per evitare che ci siano duplicati\n","cols_for_final_df = pd.Series(cols_for_final_df).drop_duplicates().tolist()\n","\n","print(\"\\nColonne da includere nel DataFrame finale:\")\n","print(cols_for_final_df)\n","\n","# Copia del DataFrame ridotto con gli eventi encodati\n","encoded_events_df = auth_reduced_df[cols_for_final_df].copy()\n","\n","print(f\"\\nDataFrame 'encoded_events_df' pronto per la creazione delle sequenze\")\n","print(f\"Forma: {encoded_events_df.shape}\")\n","print(\"Colonne in encoded_events_df:\", encoded_events_df.columns.tolist())\n","\n","# Salvataggio di encoded_events_df\n","encoded_events_filename = f\"encoded_events_df_for_sequencing_{VERSION}.csv.gz\"\n","encoded_events_path = f'{sequences_path}/{encoded_events_filename}'\n","print(f\"\\nSalvataggio di encoded_events_df in: {encoded_events_path}\")\n","try:\n","    encoded_events_df.to_csv(encoded_events_path, index=False, compression='gzip')\n","    print(\"Salvataggio di encoded_events_df completato.\")\n","except Exception as e:\n","    print(f\"Errore durante il salvataggio di encoded_events_df: {e}\")\n","\n","print(\"\\nFeature Engineering, Encoding e Etichettatura Eventi completati.\")"],"metadata":{"id":"PHxBeOEH8byq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593600208,"user_tz":-120,"elapsed":9741,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"6ecbc705-eda5-48ff-a947-f8c253ac2f43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Colonne da includere nel DataFrame finale:\n","['time', 'time_scaled_per_user', 'src_user_encoded', 'dst_user_encoded', 'src_comp_encoded', 'dst_comp_encoded', 'auth_type_encoded', 'logon_type_encoded', 'auth_orientation_encoded', 'status_encoded', 'is_anomaly']\n","\n","DataFrame 'encoded_events_df' pronto per la creazione delle sequenze\n","Forma: (780845, 11)\n","Colonne in encoded_events_df: ['time', 'time_scaled_per_user', 'src_user_encoded', 'dst_user_encoded', 'src_comp_encoded', 'dst_comp_encoded', 'auth_type_encoded', 'logon_type_encoded', 'auth_orientation_encoded', 'status_encoded', 'is_anomaly']\n","\n","Salvataggio di encoded_events_df in: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/encoded_events_df_for_sequencing_v2.csv.gz\n","Salvataggio di encoded_events_df completato.\n","\n","Feature Engineering, Encoding e Etichettatura Eventi completati.\n"]}]},{"cell_type":"markdown","source":["## Creazione delle Sequenze"],"metadata":{"id":"7rG_eiCmABav"}},{"cell_type":"markdown","source":["### Caricamento DataFrame con eventi encodati"],"metadata":{"id":"0AOWpw6aBPGv"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Definizione percorso di encoded_events\n","encoded_events_filename = f\"encoded_events_df_for_sequencing_{VERSION}.csv.gz\"\n","encoded_events_path = f'{sequences_path}/{encoded_events_filename}'\n","\n","# Caricamento del DataFrame con gli eventi encodati\n","print(f\"\\nCaricamento degli eventi encodati da: {encoded_events_path}\")\n","encoded_events_df = pd.read_csv(\n","    encoded_events_path,\n","    compression='gzip'\n","    )\n","print(f\"Caricato encoded_events_df\")\n","print(f\"Righe: {encoded_events_df.shape[0]:,}\")\n","print(f\"Colonne: {encoded_events_df.shape[1]}\")\n","print(f\"Colonne presenti ({encoded_events_df.shape[1]}):\", encoded_events_df.columns.tolist())"],"metadata":{"id":"mLWdrJ_AAWiG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593600834,"user_tz":-120,"elapsed":617,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"2d3cc52d-5a4f-4115-9b96-fd01f36208c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Caricamento degli eventi encodati da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/encoded_events_df_for_sequencing_v2.csv.gz\n","Caricato encoded_events_df\n","Righe: 780,845\n","Colonne: 11\n","Colonne presenti (11): ['time', 'time_scaled_per_user', 'src_user_encoded', 'dst_user_encoded', 'src_comp_encoded', 'dst_comp_encoded', 'auth_type_encoded', 'logon_type_encoded', 'auth_orientation_encoded', 'status_encoded', 'is_anomaly']\n"]}]},{"cell_type":"markdown","source":["### Estrazione delle sequenze e delle etichette"],"metadata":{"id":"aXs5xH6oBXUK"}},{"cell_type":"code","source":["# Logica di selezione delle feature\n","# Qui si definiscono quali colonne di `encoded_events_df` diventeranno le feature all'interno di ogni time step di una sequenza\n","# Seleziona tutte le colonne che terminano con '_encoded' e aggiunge 'time_scaled_per_user'\n","feature_cols_for_sequence = [col for col in encoded_events_df.columns if col.endswith('_encoded')] + ['time_scaled_per_user']\n","# La riga commentata escludeva 'src_user_encoded', ma la logica attuale la include\n","# feature_cols_for_sequence = [col for col in encoded_events_df.columns if col.endswith('_encoded') and col != 'src_user_encoded']\n","\n","print(f\"Parametri sequenze: WINDOW_SIZE={WINDOW_SIZE}, STRIDE={STRIDE}\")\n","print(f\"Feature per ogni step della sequenza: {feature_cols_for_sequence}\")\n","\n","# Lista delle sequenze\n","all_sequences_list = []\n","# Lista delle etichette delle sequenze (normale/anomala)\n","all_micro_labels_list = []\n","# Lista degli ID utente\n","all_user_ids_for_sequences_list = []\n","# Counter utenti processati\n","processed_user_count = 0\n","\n","# Raggruppa gli eventi per utente non mischiare attività di utenti diversi in una sequenza\n","grouped_by_user_encoded_events_df = encoded_events_df.groupby('src_user_encoded')\n","\n","print(\"\\nEstrazione delle sequenze e delle relative etichette per utente...\")\n","\n","# Itera su ciascun utente e sul DataFrame contenente le sue attività\n","for user_id, user_activity_df in grouped_by_user_encoded_events_df:\n","    processed_user_count += 1\n","    if processed_user_count % 1000 == 0:\n","        print(f\"  Elaborato utente {processed_user_count}/{len(grouped_by_user_encoded_events_df)}...\")\n","\n","    # Ordina gli eventi dell'utente per 'time' per garantire l'ordine cronologico.\n","    user_activity_df_sorted = user_activity_df.sort_values(by='time')\n","    # Estrae le feature selezionate come array NumPy\n","    user_features_np = user_activity_df_sorted[feature_cols_for_sequence].values\n","    # Estrae le etichette di anomalia a livello di evento\n","    user_event_labels_np = user_activity_df_sorted['is_anomaly'].values\n","\n","\n","    # Logica della sliding window per creare le sequenze\n","    # Itera sull'attività dell'utente con passo definito da STRIDE\n","    # Lo stop è len - WINDOW_SIZE + 1 per assicurare che l'ultima finestra abbia dimensione WINDOW_SIZE\n","    for i in range(0, len(user_features_np) - WINDOW_SIZE + 1, STRIDE):\n","        # Estrae la finestra corrente di feature (WINDOW_SIZE x num_features)\n","        sequence_features = user_features_np[i : i + WINDOW_SIZE]\n","        # Estrae le etichette di anomalia degli eventi corrispondenti a questa finestra\n","        sequence_event_labels_for_window = user_event_labels_np[i : i + WINDOW_SIZE]\n","\n","        # Etichetta la sequenza se contiene almeno un evento anomalo\n","        if sequence_event_labels_for_window.any():\n","          sequence_label = 1\n","        else:\n","          sequence_label = 0\n","\n","        # Aggiungi la sequenza come lista di liste\n","        all_sequences_list.append(sequence_features.tolist())\n","        # Aggiunge l'etichetta della sequenza\n","        all_micro_labels_list.append(sequence_label)\n","        # Aggiunge l'ID utente per questa sequenza\n","        all_user_ids_for_sequences_list.append(user_id)\n","\n","print(f\"\\nElaborazione utenti completata.\")\n","print(f\"Totale sequenze create: {len(all_sequences_list):,}\")\n","# Stampa il conteggio delle sequenze normali e anomale create\n","label_counts = pd.Series(all_micro_labels_list).value_counts()\n","print(f\"Normali={label_counts.get(0,0)}, Anomale={label_counts.get(1,0)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3TKgJvgu7Wy","executionInfo":{"status":"ok","timestamp":1748593630367,"user_tz":-120,"elapsed":29514,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"59f7d46c-9e82-4326-8079-e75d8fcfc7d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parametri sequenze: WINDOW_SIZE=60, STRIDE=1\n","Feature per ogni step della sequenza: ['src_user_encoded', 'dst_user_encoded', 'src_comp_encoded', 'dst_comp_encoded', 'auth_type_encoded', 'logon_type_encoded', 'auth_orientation_encoded', 'status_encoded', 'time_scaled_per_user']\n","\n","Estrazione delle sequenze e delle relative etichette per utente...\n","  Elaborato utente 1000/27767...\n","  Elaborato utente 2000/27767...\n","  Elaborato utente 3000/27767...\n","  Elaborato utente 4000/27767...\n","  Elaborato utente 5000/27767...\n","  Elaborato utente 6000/27767...\n","  Elaborato utente 7000/27767...\n","  Elaborato utente 8000/27767...\n","  Elaborato utente 9000/27767...\n","  Elaborato utente 10000/27767...\n","  Elaborato utente 11000/27767...\n","  Elaborato utente 12000/27767...\n","  Elaborato utente 13000/27767...\n","  Elaborato utente 14000/27767...\n","  Elaborato utente 15000/27767...\n","  Elaborato utente 16000/27767...\n","  Elaborato utente 17000/27767...\n","  Elaborato utente 18000/27767...\n","  Elaborato utente 19000/27767...\n","  Elaborato utente 20000/27767...\n","  Elaborato utente 21000/27767...\n","  Elaborato utente 22000/27767...\n","  Elaborato utente 23000/27767...\n","  Elaborato utente 24000/27767...\n","  Elaborato utente 25000/27767...\n","  Elaborato utente 26000/27767...\n","  Elaborato utente 27000/27767...\n","\n","Elaborazione utenti completata.\n","Totale sequenze create: 230,504\n","Normali=228102, Anomale=2402\n"]}]},{"cell_type":"markdown","source":["### Conversione in array NumPy e salvataggio"],"metadata":{"id":"Y06_RIC4BbSU"}},{"cell_type":"code","source":["import os\n","\n","print(\"Conversione delle liste di sequenze e etichette in array NumPy...\")\n","\n","# Conversioni delle liste in array NumPy\n","# dtype=np.float64 per X_final_sequences è importante perché una delle feature ('time_scaled_per_user') è float\n","X_final_sequences = np.array(all_sequences_list, dtype=np.float64)\n","# Etichette intere (0 o 1)\n","y_final_labels = np.array(all_micro_labels_list, dtype=np.int64)\n","# ID utenti\n","user_ids_final_array = np.array(all_user_ids_for_sequences_list)\n","\n","print(f\"Forma finale delle sequenze (X_final_sequences): {X_final_sequences.shape}\") # (num_sequenze, WINDOW_SIZE, num_feature_per_step=9)\n","print(f\"Forma finale delle etichette (y_final_labels): {y_final_labels.shape}\") # (num_sequenze,)\n","print(f\"Forma finale degli ID utente (user_ids_final_array): {user_ids_final_array.shape}\") # (num_sequenze,)\n","\n","# Definizione percorso delle sequenze\n","sequences_folder_name = f\"sequences_{VERSION}_W{WINDOW_SIZE}_S{STRIDE}\"\n","sequences_output_path = f'{sequences_path}/{sequences_folder_name}'\n","\n","# Creazione cartella (se non esiste)\n","os.makedirs(sequences_output_path, exist_ok=True)\n","print(f\"\\nI file verranno salvati in: {sequences_output_path}\")\n","\n","# Definizione nomi X, y e ID utenti\n","X_filename = f\"X_sequences_{VERSION}.npy\"\n","y_filename = f\"y_labels_{VERSION}.npy\"\n","user_ids_filename = f\"user_ids_for_sequences_{VERSION}.npy\"\n","\n","# Definizione percorsi X, y e ID utenti\n","X_path = f'{sequences_output_path}/{X_filename}'\n","y_path = f'{sequences_output_path}/{y_filename}'\n","user_ids_path = f'{sequences_output_path}/{user_ids_filename}'\n","\n","# Salvataggi\n","print(f\"Salvataggio di X_final_sequences in: {X_path}\")\n","np.save(X_path, X_final_sequences)\n","print(f\"Salvataggio di y_final_labels in: {y_path}\")\n","np.save(y_path, y_final_labels)\n","print(f\"Salvataggio di user_ids_final_array in: {user_ids_path}\")\n","np.save(user_ids_path, user_ids_final_array)\n","print(\"Salvataggio completato.\")\n","\n","print(\"\\nCreazione e salvataggio Sequenze completato\")"],"metadata":{"id":"M9bh6O9WACiz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"76038232-c39f-4138-b4e7-5f300174d823","executionInfo":{"status":"ok","timestamp":1748593645165,"user_tz":-120,"elapsed":14755,"user":{"displayName":"Antonio","userId":"04509716177869496437"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Conversione delle liste di sequenze e etichette in array NumPy...\n","Forma finale delle sequenze (X_final_sequences): (230504, 60, 9)\n","Forma finale delle etichette (y_final_labels): (230504,)\n","Forma finale degli ID utente (user_ids_final_array): (230504,)\n","\n","I file verranno salvati in: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v2_W60_S1\n","Salvataggio di X_final_sequences in: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v2_W60_S1/X_sequences_v2.npy\n","Salvataggio di y_final_labels in: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v2_W60_S1/y_labels_v2.npy\n","Salvataggio di user_ids_final_array in: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v2_W60_S1/user_ids_for_sequences_v2.npy\n","Salvataggio completato.\n","\n","Creazione e salvataggio Sequenze completato\n"]}]},{"cell_type":"markdown","source":["## Suddivisione Finale del Dataset (Train/Validation/Test)"],"metadata":{"id":"c7ljqGo-Eikv"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","import os\n","\n","# Definizione percorso cartella delle sequenze\n","sequences_folder_name = f\"sequences_{VERSION}_W{WINDOW_SIZE}_S{STRIDE}\"\n","sequences_dir = f'{sequences_path}/{sequences_folder_name}'\n","\n","# Definizione nomi dei file .npy\n","X_filename = f\"X_sequences_{VERSION}.npy\"\n","y_filename = f\"y_labels_{VERSION}.npy\"\n","user_ids_filename = f\"user_ids_for_sequences_{VERSION}.npy\"\n","\n","# Definizione percorsi dei file .npy\n","path_X_all = f'{sequences_dir}/{X_filename}'\n","path_y_all = f'{sequences_dir}/{y_filename}'\n","path_user_ids_all = f'{sequences_dir}/{user_ids_filename}'\n","\n","print(f\"Caricamento delle sequenze da: {sequences_dir}\")\n","\n","# Caricamenti\n","X_all_sequences = np.load(path_X_all)\n","y_all_micro_labels = np.load(path_y_all)\n","user_ids_per_sequence = np.load(path_user_ids_all)\n","\n","print(f\"\\nCaricamento completato.\")\n","\n","print(f\"Forma di X_all_sequences: {X_all_sequences.shape}\")\n","print(f\"Forma di y_all_micro_labels: {y_all_micro_labels.shape}\")\n","print(f\"Forma di user_ids_per_sequence: {user_ids_per_sequence.shape}\")\n","\n","unique_labels_all, counts_labels_all = np.unique(y_all_micro_labels, return_counts=True)\n","print(f\"Conteggio etichette totali nelle sequenze: {dict(zip(unique_labels_all, counts_labels_all))}\")\n","\n","# ID utente unici da tutte le sequenze\n","# La suddivisione avverrà a livello di utente\n","unique_users = np.unique(user_ids_per_sequence)\n","print(f\"Numero di utenti unici: {len(unique_users)}\")\n","\n","# Primo split (Training - Validation/Test)\n","users_train_ids, users_temp_pool_ids = train_test_split(\n","    unique_users,\n","    test_size=0.40,\n","    random_state=42,\n","    shuffle=False\n",")\n","\n","# Secondo split (Validation - Test)\n","users_val_ids, users_test_ids = train_test_split(\n","    users_temp_pool_ids,\n","    test_size=0.50,\n","    random_state=42,\n","    shuffle=False\n",")\n","\n","print(f\"\\nSuddivisione utenti completata:\")\n","print(f\"  Utenti per Training: {len(users_train_ids)}\")\n","print(f\"  Utenti per Validation: {len(users_val_ids)}\")\n","print(f\"  Utenti per Test: {len(users_test_ids)}\")"],"metadata":{"id":"HO3keS62Ewfh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593650472,"user_tz":-120,"elapsed":5303,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"5416e628-cecf-4918-b3d3-a8d7ad9c07da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Caricamento delle sequenze da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v2_W60_S1\n","\n","Caricamento completato.\n","Forma di X_all_sequences: (230504, 60, 9)\n","Forma di y_all_micro_labels: (230504,)\n","Forma di user_ids_per_sequence: (230504,)\n","Conteggio etichette totali nelle sequenze: {np.int64(0): np.int64(228102), np.int64(1): np.int64(2402)}\n","Numero di utenti unici: 1225\n","\n","Suddivisione utenti completata:\n","  Utenti per Training: 735\n","  Utenti per Validation: 245\n","  Utenti per Test: 245\n"]}]},{"cell_type":"markdown","source":["### Riepilogo e salvataggio dei set finali"],"metadata":{"id":"Y8AClTqnGpZi"}},{"cell_type":"code","source":["# Isolamento delle sequenze normali per il Training set\n","# Crea una maschera booleana per selezionare le sequenze appartenenti agli utenti di training\n","train_user_mask = np.isin(user_ids_per_sequence, users_train_ids)\n","# Estrae tutte le sequenze e le etichette degli utenti di training\n","X_from_train_users = X_all_sequences[train_user_mask]\n","y_from_train_users = y_all_micro_labels[train_user_mask]\n","# Crea il set di training finale contenente SOLO le sequenze NORMALI\n","X_train = X_from_train_users[y_from_train_users == 0]\n","y_train = y_from_train_users[y_from_train_users == 0]\n","\n","# Validation Set (tutte le sequenze da utenti di validation)\n","val_user_mask = np.isin(user_ids_per_sequence, users_val_ids)\n","X_val = X_all_sequences[val_user_mask]\n","y_val = y_all_micro_labels[val_user_mask]\n","\n","# Test Set (tutte le sequenze da utenti di test)\n","test_user_mask = np.isin(user_ids_per_sequence, users_test_ids)\n","X_test = X_all_sequences[test_user_mask]\n","y_test = y_all_micro_labels[test_user_mask]\n","\n","print(f\"Riepilogo Forme Finali dei Set Suddivisi\")\n","print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","if y_train.size > 0: print(f\"  Etichette y_train: Normali={np.sum(y_train == 0)}, Anomale={np.sum(y_train == 1)}\")\n","\n","print(f\"X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n","if y_val.size > 0: print(f\"  Etichette y_val:   Normali={np.sum(y_val == 0)}, Anomale={np.sum(y_val == 1)}\")\n","\n","print(f\"X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n","if y_test.size > 0: print(f\"  Etichette y_test:  Normali={np.sum(y_test == 0)}, Anomale={np.sum(y_test == 1)}\")\n","\n","# Calcola e stampa la distribuzione delle anomalie tra il validation e il test set\n","anomalies_in_val = np.sum(y_val == 1)\n","anomalies_in_test = np.sum(y_test == 1)\n","total_anomalies_in_val_test = anomalies_in_val + anomalies_in_test\n","\n","print(f\"\\nDistribuzione delle anomalie (basata sulla separazione per utente):\")\n","if total_anomalies_in_val_test > 0:\n","    percent_anomalies_in_val = (anomalies_in_val / total_anomalies_in_val_test) * 100\n","    percent_anomalies_in_test = (anomalies_in_test / total_anomalies_in_val_test) * 100\n","    print(f\"  Sequenze anomale nel Validation Set: {anomalies_in_val} ({percent_anomalies_in_val:.2f}%)\")\n","    print(f\"  Sequenze anomale nel Test Set: {anomalies_in_test} ({percent_anomalies_in_test:.2f}%)\")\n","    print(f\"  Totale sequenze anomale in Val+Test: {total_anomalies_in_val_test}\")\n","else:\n","    print(\"  Nessuna equenza anomala trovata nei set di validazione o test.\")"],"metadata":{"id":"mdesQv_JEpel","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593652914,"user_tz":-120,"elapsed":2402,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"d38267f2-61cd-451a-df6c-0716f8bba374"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Riepilogo Forme Finali dei Set Suddivisi\n","X_train: (153184, 60, 9), y_train: (153184,)\n","  Etichette y_train: Normali=153184, Anomale=0\n","X_val:   (37641, 60, 9), y_val:   (37641,)\n","  Etichette y_val:   Normali=37170, Anomale=471\n","X_test:  (39241, 60, 9), y_test:  (39241,)\n","  Etichette y_test:  Normali=37748, Anomale=1493\n","\n","Distribuzione delle anomalie (basata sulla separazione per utente):\n","  Sequenze anomale nel Validation Set: 471 (23.98%)\n","  Sequenze anomale nel Test Set: 1493 (76.02%)\n","  Totale sequenze anomale in Val+Test: 1964\n"]}]},{"cell_type":"markdown","source":["### Salvataggio dei set di training, validation e test"],"metadata":{"id":"IduFosOkqpq2"}},{"cell_type":"code","source":["# Definizione percorso cartella degli split\n","user_split_strategy_folder_name = f\"user_split_W{WINDOW_SIZE}_S{STRIDE}\"\n","user_split_output_dir = f'{sequences_path}/{sequences_folder_name}/{user_split_strategy_folder_name}'\n","os.makedirs(user_split_output_dir, exist_ok=True)\n","print(f\"I set divisi per utente verranno salvati in: {user_split_output_dir}\")\n","\n","# Salvataggio degli array X e y per ciascun set\n","try:\n","    np.save(f'{user_split_output_dir}/X_train_normal_only.npy', X_train)\n","    np.save(f'{user_split_output_dir}/y_train_normal_only.npy', y_train)\n","    np.save(f'{user_split_output_dir}/X_val_user_split.npy', X_val)\n","    np.save(f'{user_split_output_dir}/y_val_user_split.npy', y_val)\n","    np.save(f'{user_split_output_dir}/X_test_user_split.npy', X_test)\n","    np.save(f'{user_split_output_dir}/y_test_user_split.npy', y_test)\n","    print(\"Tutti i set di dati finali (suddivisi per utente) sono stati salvati con successo.\")\n","except Exception as e:\n","    print(f\"Si è verificato un errore durante il salvataggio dei set di dati finali: {e}\")"],"metadata":{"id":"BWigA48OGvGE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748593677350,"user_tz":-120,"elapsed":24429,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"d991251f-6aca-41b8-cda5-312820940bad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I set divisi per utente verranno salvati in: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v2_W60_S1/user_split_W60_S1\n","Tutti i set di dati finali (suddivisi per utente) sono stati salvati con successo.\n"]}]}]}