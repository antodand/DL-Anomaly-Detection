{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMdjHBXer8THJL8O0CoIRls"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Import necessari\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import numpy as np\n","import pandas as pd\n","import json\n","import os\n","import math\n","from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive"],"metadata":{"id":"4raUTkIfbCLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variabili Globali\n","VERSION = \"v1\"\n","WINDOW_SIZE = 60\n","STRIDE = 1\n","\n","# Iperparametri del modello Transformer e di addestramento\n","EMBEDDING_DIM = 16\n","D_MODEL_TRANSFORMER = 128           # Dimensione interna dei vettori nel Transformer (divisibile per NHEAD_TRANSFORMER)\n","NHEAD_TRANSFORMER = 4               # Numero di \"teste\" nel Multi-Head Attention\n","NUM_ENCODER_LAYERS_TRANSFORMER = 2  # Numero di Transformer Encoder Layers impilati\n","DIM_FEEDFORWARD_TRANSFORMER = 512   # Dimensione del layer feed-forward interno a ogni Transformer Encoder Layer\n","OUTPUT_DIM = 1                      # Output binario (un logit per la classificazione anomalia/normale)\n","\n","DROPOUT_TRANSFORMER = 0.2     # Tasso di dropout\n","LEARNING_RATE = 1e-4          # Tasso di apprendimento per l'optimizer\n","WEIGHT_DECAY = 1e-4           # Regularizzazione L2 (weight decay). Hai aumentato questo valore\n","BATCH_SIZE = 32               # Dimensione dei batch\n","NUM_EPOCHS = 1               # Numero massimo di epoche di addestramento\n","PATIENCE_EARLY_STOPPING = 3   # Numero di epoche senza miglioramento della Val Loss prima di fermarsi\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Utilizzo del dispositivo: {DEVICE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QdLEkK3_bH7z","executionInfo":{"status":"ok","timestamp":1748651256339,"user_tz":-120,"elapsed":30,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"b5865f67-7377-4705-9373-2c152e6a609e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Utilizzo del dispositivo: cpu\n"]}]},{"cell_type":"code","source":["# Montaggio Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Percorsi\n","base_path = '/content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV'\n","sequences_path = f'{base_path}/Sequences'\n","embeddings_path = f'{base_path}/Embeddings'\n","models_path = f'{base_path}/Models'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAD0wHIgbNOp","executionInfo":{"status":"ok","timestamp":1748651262264,"user_tz":-120,"elapsed":5901,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"7809fc53-4375-489d-85bd-e0b1a625716b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Percorsi dei file NumPy\n","sequences_folder_name = f\"sequences_{VERSION}_W{WINDOW_SIZE}_S{STRIDE}\"\n","user_split_strategy_folder_name = f\"user_split_W{WINDOW_SIZE}_S{STRIDE}\"\n","split_data_dir = f'{sequences_path}/{sequences_folder_name}/{user_split_strategy_folder_name}'\n","\n","X_train_path = f'{split_data_dir}/X_train_normal_only.npy'\n","y_train_path = f'{split_data_dir}/y_train_normal_only.npy'\n","X_val_path = f'{split_data_dir}/X_val_user_split.npy'\n","y_val_path = f'{split_data_dir}/y_val_user_split.npy'\n","X_test_path = f'{split_data_dir}/X_test_user_split.npy'\n","y_test_path = f'{split_data_dir}/y_test_user_split.npy'\n","\n","# Caricamento degli array NumPy preprocessati\n","print(f\"Caricamento X_train da: {X_train_path}\")\n","X_train_np = np.load(X_train_path)\n","print(f\"Caricamento y_train da: {y_train_path}\")\n","y_train_np = np.load(y_train_path)\n","\n","print(f\"Caricamento X_val da: {X_val_path}\")\n","X_val_np = np.load(X_val_path)\n","print(f\"Caricamento y_val da: {y_val_path}\")\n","y_val_np = np.load(y_val_path)\n","\n","print(f\"Caricamento X_test da: {X_test_path}\")\n","X_test_np = np.load(X_test_path)\n","print(f\"Caricamento y_test da: {y_test_path}\")\n","y_test_np = np.load(y_test_path)\n","\n","# Stampa delle dimensioni dei dati caricati\n","print(f\"\\nForme dei dati caricati:\")\n","print(f\"X_train: {X_train_np.shape}, y_train: {y_train_np.shape}\")\n","print(f\"X_val:   {X_val_np.shape},  y_val:   {y_val_np.shape}\")\n","print(f\"X_test:  {X_test_np.shape}, y_test:  {y_test_np.shape}\")\n","\n","# File metadati embedding\n","# Percorso al file JSON con i dati degli embedding (vocab sizes)\n","embedding_data_path = f'{embeddings_path}/embedding_data_{VERSION}.json'\n","\n","# Carica embedding_data per ottenere le vocab_sizes\n","with open(embedding_data_path, 'r') as f:\n","    embedding_data_json = json.load(f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6g9sXlNeZ9u","executionInfo":{"status":"ok","timestamp":1748651277028,"user_tz":-120,"elapsed":14768,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"9248ffdb-9d47-4e0e-f839-80dae1f3ef97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Caricamento X_train da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v1_W60_S1/user_split_W60_S1/X_train_normal_only.npy\n","Caricamento y_train da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v1_W60_S1/user_split_W60_S1/y_train_normal_only.npy\n","Caricamento X_val da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v1_W60_S1/user_split_W60_S1/X_val_user_split.npy\n","Caricamento y_val da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v1_W60_S1/user_split_W60_S1/y_val_user_split.npy\n","Caricamento X_test da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v1_W60_S1/user_split_W60_S1/X_test_user_split.npy\n","Caricamento y_test da: /content/drive/MyDrive/Colab Notebooks/Deep_Learning_2025_IV/Sequences/sequences_v1_W60_S1/user_split_W60_S1/y_test_user_split.npy\n","\n","Forme dei dati caricati:\n","X_train: (153184, 60, 9), y_train: (153184,)\n","X_val:   (26954, 60, 9),  y_val:   (26954,)\n","X_test:  (49928, 60, 9), y_test:  (49928,)\n"]}]},{"cell_type":"code","source":["# Determina le feature da usare per le sequenze\n","# Lista delle feature usate per creare X_final_sequences\n","# Questa lista DEVE corrispondere a 'feature_cols_for_sequences' nel codice di preprocessing\n","categorical_feature_names = [\n","    'src_user',\n","    'dst_user',\n","    'src_comp',\n","    'dst_comp',\n","    'auth_type',\n","    'logon_type',\n","    'auth_orientation',\n","    'status'\n","]\n","\n","# Nomi delle feature continue\n","continuous_feature_names = ['time_scaled_per_user']\n","\n","# Determina il numero di feature categoriche e continue\n","num_categorical_features = len(categorical_feature_names)\n","num_continuous_features = len(continuous_feature_names)\n","\n","print(f\"\\nConfigurazione Feature:\")\n","print(f\"Nomi feature categoriche: {categorical_feature_names} (Numero: {num_categorical_features})\")\n","print(f\"Nomi feature continue: {continuous_feature_names} (Numero: {num_continuous_features})\")\n","\n","raw_concatenated_feature_dim = (num_categorical_features * EMBEDDING_DIM) + num_continuous_features\n","print(f\"Dimensione feature grezza concatenata (prima della proiezione a d_model): {raw_concatenated_feature_dim}\")\n","\n","# Verifica la coerenza con X_train_np.shape[2]\n","# Il numero totale di feature deve corrispondere al numero di feature\n","# effettivamente presenti nei dati caricati\n","expected_total_features = num_categorical_features + num_continuous_features\n","if X_train_np.shape[2] != expected_total_features:\n","    print(f\"ERRORE: Numero totale feature ({expected_total_features}) nei dati caricati X_train.shape[2] ({X_train_np.shape[2]})\")\n","    print(\"non corrisponde alla somma di num_categorical_features e num_continuous_features.\")\n","else:\n","    print(f\"OK: Numero totale feature ({expected_total_features}) corrisponde a X_train.shape[2].\")\n","\n","# Estrai le vocab_sizes solo per le feature categoriche\n","# Crea la lista 'ordered_vocab_sizes' che verrà passata al modello\n","ordered_vocab_sizes = []\n","# Itera su 'categorical_feature_names'\n","for original_col_name in categorical_feature_names:\n","    vocab_size_key = f'{original_col_name}_vocab_size'\n","    if vocab_size_key in embedding_data_json:\n","        ordered_vocab_sizes.append(embedding_data_json[vocab_size_key])\n","    else: # Fallback se la chiave non è trovata (improbabile)\n","        vocab_size_key_alt = f'{original_col_name.replace(\"_encoded\", \"\")}_vocab_size'\n","        if vocab_size_key_alt in embedding_data_json:\n","             ordered_vocab_sizes.append(embedding_data_json[vocab_size_key_alt])\n","        else:\n","            raise ValueError(f\"Vocab size non trovata per {original_col_name} in {embedding_data_path}\")\n","print(f\"Dimensioni dei vocabolari: {ordered_vocab_sizes} (Numero: {len(ordered_vocab_sizes)})\")\n","\n","# Verifica la compatibilità tra D_MODEL_TRANSFORMER e NHEAD_TRANSFORMER\n","if D_MODEL_TRANSFORMER % NHEAD_TRANSFORMER != 0:\n","    raise ValueError(f\"D_MODEL_TRANSFORMER ({D_MODEL_TRANSFORMER}) deve essere divisibile per NHEAD_TRANSFORMER ({NHEAD_TRANSFORMER})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ku1m_G9wfH47","executionInfo":{"status":"ok","timestamp":1748651277051,"user_tz":-120,"elapsed":20,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"a618b91d-77b0-4f28-9657-e302938efb54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Configurazione Feature:\n","Nomi feature categoriche: ['src_user', 'dst_user', 'src_comp', 'dst_comp', 'auth_type', 'logon_type', 'auth_orientation', 'status'] (Numero: 8)\n","Nomi feature continue: ['time_scaled_per_user'] (Numero: 1)\n","Dimensione feature grezza concatenata (prima della proiezione a d_model): 129\n","OK: Numero totale feature (9) corrisponde a X_train.shape[2].\n","Dimensioni dei vocabolari: [27767, 29962, 13078, 11642, 6, 10, 7, 2] (Numero: 8)\n"]}]},{"cell_type":"code","source":["# Definizione della classe PositionalEncoding\n","# Necessaria perché aggiunge informazioni sulla posizione degli elementi nella sequenza,\n","# dato che il Transformer di per sé non ha una nozione di ordine sequenziale\n","class PositionalEncoding(nn.Module):\n","    def __init__(\n","        self,\n","        d_model,      # Dimensione degli embedding di input\n","        dropout=0.1,  # Dropout rate\n","        max_len=5000  # Lunghezza massima della sequenza per cui precalcolare il positional encoding\n","        ):\n","\n","        super(PositionalEncoding, self).__init__()\n","\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Crea un tensore di zeri\n","        pe = torch.zeros(max_len, d_model)\n","\n","        # Vettore di posizioni (0, 1, ..., max_len-1)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","\n","        # Calcola i termini divisori per le funzioni seno e coseno, basati sulla formula originale del Transformer\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        # Applica sin alle posizioni pari\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        # Applica cos alle posizioni dispari\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        # Aggiunge una dimensione per il batch (1, max_len, d_model)\n","        pe = pe.unsqueeze(0)\n","\n","        # Registra 'pe' come parte dello stato del modulo, ma non come parametro da addestrare\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, d_model)\n","        # Aggiunge il positional encoding (selezionando fino a seq_len) all'input x\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)\n","\n","# Definizione della classe del modello Transformer\n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_sizes,\n","        embedding_dim,\n","        num_continuous_features,\n","        d_model,\n","        nhead,\n","        num_encoder_layers,\n","        dim_feedforward,\n","        output_dim,\n","        dropout_rate,\n","        seq_len\n","        ):\n","\n","        super(Transformer, self).__init__()\n","\n","        self.num_categorical_features = len(vocab_sizes)\n","        self.num_continuous_features = num_continuous_features\n","        self.embedding_dim = embedding_dim\n","\n","        # Layer di Embedding\n","        self.embeddings = nn.ModuleList([\n","            nn.Embedding(num_embeddings=v_size, embedding_dim=embedding_dim) for v_size in vocab_sizes\n","        ])\n","\n","        # Dimensione dell'input dopo aver concatenato embedding e feature continue\n","        raw_concatenated_dim = (self.num_categorical_features * self.embedding_dim) + self.num_continuous_features\n","\n","        # Layer di Proiezione Lineare per mappare la dimensione dell'input concatenato a d_model\n","        # Necessario se raw_concatenated_dim != d_model o per rendere d_model divisibile per nhead\n","        self.input_projection = nn.Linear(raw_concatenated_dim, d_model)\n","\n","        # Istanzia il layer di Positional Encoding\n","        self.pos_encoder = PositionalEncoding(d_model, dropout_rate, max_len=seq_len + 5) # max_len definito più grande di seq_len\n","\n","        # Creazione di un singolo Transformer Encoder Layer\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=d_model, # Dimensione dell'input per il layer (e output)\n","            nhead=nhead, # Numero di teste nell'attention multi-head\n","            dim_feedforward=dim_feedforward, # Dimensione del layer feed-forward interno\n","            dropout=dropout_rate, # Dropout rate\n","            batch_first=True # batch_first=True aspetta input (batch, seq, feature)\n","        )\n","        # Stack di Transformer Encoder Layers\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n","\n","        # Dropout prima del layer lineare finale.\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        # Layer lineare che mappa l'output aggregato del Transformer a output_dim (1 logit)\n","        self.fc_classifier = nn.Linear(d_model, output_dim)\n","\n","        print(f\"Transformer Model Initialized:\")\n","        print(f\"  Raw concatenated feature dim: {raw_concatenated_dim}\")\n","        print(f\"  Projected to d_model: {d_model}\")\n","        print(f\"  Num heads: {nhead}, Num encoder layers: {num_encoder_layers}, Dim feedforward: {dim_feedforward}\")\n","\n","    # Metodo per processare le feature di input grezze (embedding + concatenazione)\n","    def _process_input_features(self, x):\n","        x_categorical = x[:, :, :self.num_categorical_features].long()\n","        x_continuous = x[:, :, self.num_categorical_features:]\n","        embedded_features_list = [self.embeddings[i](x_categorical[:, :, i]) for i in range(self.num_categorical_features)]\n","        if self.num_categorical_features > 0:\n","            processed_features = torch.cat(embedded_features_list, dim=2)\n","            if self.num_continuous_features > 0:\n","                processed_features = torch.cat((processed_features, x_continuous.float()), dim=2)\n","        elif self.num_continuous_features > 0:\n","            processed_features = x_continuous.float()\n","        else:\n","            raise ValueError(\"Il modello deve avere almeno una feature.\")\n","        return processed_features\n","\n","    def forward(self, src):\n","        # src shape: (batch, seq_len, num_total_raw_features)\n","\n","        processed_src = self._process_input_features(src) # (batch, seq_len, raw_concatenated_dim)\n","        projected_src = self.input_projection(processed_src) # (batch, seq_len, d_model)\n","        src_with_pos = self.pos_encoder(projected_src) # (batch, seq_len, d_model)\n","\n","        # Passaggio attraverso i layer Transformer Encoder\n","        # Non usiamo maschere (src_mask, src_key_padding_mask) assumendo che tutte le sequenze\n","        # nel batch abbiano la stessa lunghezza effettiva (WINDOW_SIZE)\n","        # e che ogni token possa attendere a tutti gli altri\n","        memory = self.transformer_encoder(src_with_pos) # (batch, seq_len, d_model)\n","\n","        # Pooling per ottenere una rappresentazione fissa della sequenza\n","        pooled_output = memory.mean(dim=1) # (batch, d_model)\n","\n","        pooled_output_dropped = self.dropout(pooled_output)\n","        output_logits = self.fc_classifier(pooled_output_dropped) # (batch, output_dim)\n","\n","        # Restituisce i logits. Il secondo valore (None) è per mantenere la coerenza con la firma di\n","        # get_predictions_and_labels_attention se usata, che si aspetta una tupla\n","        return output_logits, None"],"metadata":{"id":"9XbocMmvhhZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Istanziazione del modello Transformer\n","print(\"\\nInizializzazione del Modello Transformer\")\n","model = Transformer(\n","    vocab_sizes=ordered_vocab_sizes,\n","    embedding_dim=EMBEDDING_DIM,\n","    num_continuous_features=num_continuous_features,\n","    d_model=D_MODEL_TRANSFORMER,\n","    nhead=NHEAD_TRANSFORMER,\n","    num_encoder_layers=NUM_ENCODER_LAYERS_TRANSFORMER,\n","    dim_feedforward=DIM_FEEDFORWARD_TRANSFORMER,\n","    output_dim=OUTPUT_DIM,\n","    dropout_rate=DROPOUT_TRANSFORMER,\n","    seq_len=WINDOW_SIZE\n",")\n","model.to(DEVICE)\n","print(\"\\nModello Transformer istanziato.\")\n","print(model)\n","\n","# Loss function\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","\n","print(\"\\nLoss function e optimizer definiti per Transformer.\")\n","\n","# Creazione DataLoaders\n","# Conversione degli array NumPy in Tensori PyTorch\n","# Gli input X devono essere LongTensor per i layer di embedding\n","X_train_tensor = torch.from_numpy(X_train_np).float()\n","y_train_tensor = torch.from_numpy(y_train_np).float() # Per BCEWithLogitsLoss\n","\n","X_val_tensor = torch.from_numpy(X_val_np).float()\n","y_val_tensor = torch.from_numpy(y_val_np).float()\n","\n","X_test_tensor = torch.from_numpy(X_test_np).float()\n","y_test_tensor = torch.from_numpy(y_test_np).float()\n","\n","# Creazione di TensorDataset, che incapsula tensori con la stessa prima dimensione\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","\n","# Creazione dei DataLoader, che gestiscono il batching, lo shuffle e il caricamento parallelo dei dati\n","# shuffle=True per il train_loader per ridurre la varianza del gradiente e migliorare la generalizzazione\n","# drop_last=True per il train_loader per evitare batch di dimensioni diverse che potrebbero dare problemi con alcuni layer\n","# shuffle=False per val_loader e test_loader perché l'ordine non deve cambiare per una valutazione consistente\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True) # Shuffle per il training\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","print(\"\\nDataLoaders creati.\")\n","print(f\"Numero di batch in train_loader: {len(train_loader)}\")\n","print(f\"Numero di batch in val_loader: {len(val_loader)}\")\n","\n","# Esempio di un batch dal train_loader\n","# Utile per verificare le dimensioni dei tensori prodotti dal DataLoader\n","if len(train_loader) > 0: # Controllo per evitare errori se il loader è vuoto\n","    dataiter = iter(train_loader)\n","    sample_x, sample_y = next(dataiter)\n","    print(f\"\\nForma di un batch di input X: {sample_x.shape}\") # (BATCH_SIZE, WINDOW_SIZE, num_total_features=9)\n","    print(f\"Forma di un batch di output y: {sample_y.shape}\") # (BATCH_SIZE)\n","else:\n","    print(\"Train loader è vuoto.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-t99XAmk8IS","executionInfo":{"status":"ok","timestamp":1748651278882,"user_tz":-120,"elapsed":1806,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"outputId":"2481cebd-f20d-4265-bcb9-380f46b43a11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Inizializzazione del Modello Transformer\n","Transformer Model Initialized:\n","  Raw concatenated feature dim: 129\n","  Projected to d_model: 128\n","  Num heads: 4, Num encoder layers: 2, Dim feedforward: 512\n","\n","Modello Transformer istanziato.\n","Transformer(\n","  (embeddings): ModuleList(\n","    (0): Embedding(27767, 16)\n","    (1): Embedding(29962, 16)\n","    (2): Embedding(13078, 16)\n","    (3): Embedding(11642, 16)\n","    (4): Embedding(6, 16)\n","    (5): Embedding(10, 16)\n","    (6): Embedding(7, 16)\n","    (7): Embedding(2, 16)\n","  )\n","  (input_projection): Linear(in_features=129, out_features=128, bias=True)\n","  (pos_encoder): PositionalEncoding(\n","    (dropout): Dropout(p=0.2, inplace=False)\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-1): 2 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=512, bias=True)\n","        (dropout): Dropout(p=0.2, inplace=False)\n","        (linear2): Linear(in_features=512, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.2, inplace=False)\n","        (dropout2): Dropout(p=0.2, inplace=False)\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc_classifier): Linear(in_features=128, out_features=1, bias=True)\n",")\n","\n","Loss function e optimizer definiti per Transformer.\n","\n","DataLoaders creati.\n","Numero di batch in train_loader: 4787\n","Numero di batch in val_loader: 843\n","\n","Forma di un batch di input X: torch.Size([32, 60, 9])\n","Forma di un batch di output y: torch.Size([32])\n"]}]},{"cell_type":"code","source":["# Ciclo di Training per il Transformer\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","best_model_weights_path = f\"{models_path}/transformer_best_model_weights.pt\"\n","\n","train_losses_log = []\n","val_losses_log = []\n","\n","print(f\"Inizio training Transformer per {NUM_EPOCHS} epoche (Early Stopping, patience={PATIENCE_EARLY_STOPPING})\")\n","\n","for epoch in range(NUM_EPOCHS):\n","    model.train() # Mette il modello in modalità training\n","    train_loss_accum = 0\n","\n","    # Itera sui batch del training set\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","\n","        # Azzera i gradienti accumulati dal batch precedente\n","        optimizer.zero_grad()\n","\n","        # Forward pass: ottiene i logits dal modello\n","        # Il secondo output (stato nascosto) dell'LSTM non è usato qui\n","        outputs_logits, _ = model(inputs) # AI: Il forward restituisce (logits, None).\n","\n","        # Calcola la loss\n","        # .squeeze() rimuove la dimensione superflua (es. da [64,1] a [64])\n","        loss = criterion(outputs_logits.squeeze(), labels)\n","\n","        # Calcola i gradienti della loss rispetto ai parametri del modello (backpropagation)\n","        loss.backward()\n","\n","        # Aggiorna i pesi del modello usando i gradienti calcolati\n","        optimizer.step()\n","\n","        # Accumula la loss del batch\n","        train_loss_accum += loss.item()\n","\n","    # Calcolo loss media di training per l'epoca\n","    avg_train_loss = train_loss_accum / len(train_loader)\n","    train_losses_log.append(avg_train_loss)\n","\n","    # VALIDATION LOOP\n","    model.eval() # Mette il modello in modalità valutazione.\n","    val_loss_accum = 0\n","\n","    with torch.no_grad(): # Disabilita il calcolo dei gradienti durante la validazione\n","        for inputs_val, labels_val_batch in val_loader:\n","            inputs_val, labels_val_gpu = inputs_val.to(DEVICE), labels_val_batch.to(DEVICE)\n","\n","            outputs_val_logits, _ = model(inputs_val)\n","\n","            val_loss = criterion(outputs_val_logits.squeeze(), labels_val_gpu)\n","            val_loss_accum += val_loss.item()\n","\n","    avg_val_loss = val_loss_accum / len(val_loader)\n","\n","    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n","\n","    # Logica di Early Stopping\n","    # Se la Val Loss attuale è migliore della migliore finora\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        # Salva i pesi del modello\n","        torch.save(model.state_dict(), best_model_weights_path)\n","        print(f\"  Miglioramento Val Loss TF: {best_val_loss:.6f}. Modello salvato.\")\n","        # Resetta il contatore delle epoche senza miglioramento\n","        epochs_no_improve = 0\n","    else: # Se la Val Loss non è migliorata\n","        epochs_no_improve += 1\n","        print(f\"  Nessun miglioramento Val Loss TF per {epochs_no_improve} epoche.\")\n","    # Se la Val Loss non migliora per 'patience' epoche consecutive\n","    if epochs_no_improve >= PATIENCE_EARLY_STOPPING:\n","        print(f\"Early stopping Transformer attivato dopo {epoch+1} epoche.\")\n","        break # Interrompi il training\n","\n","print(\"\\nTraining Transformer completato.\")\n","\n","# Plot delle curve di loss (diagnostica)\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_losses_log, label='Training Loss Transformer')\n","plt.plot(val_losses_log, label='Validation Loss Transformer')\n","plt.title('Curve di Loss Transformer');\n","plt.xlabel('Epoca');\n","plt.ylabel('Loss');\n","plt.legend();\n","plt.grid(True);\n","plt.show()"],"metadata":{"id":"lM_q-hCqmjsh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import torch.nn.functional as F\n","\n","def get_predictions_and_labels(model, data_loader, device_eval):\n","    model.eval()  # Mette il modello in modalità valutazione\n","    all_labels_eval = []\n","    all_probs_eval = []\n","\n","    with torch.no_grad():  # Disabilita il calcolo dei gradienti\n","        for inputs_eval, labels_eval in data_loader:\n","            inputs_eval = inputs_eval.to(device_eval)\n","\n","            # Ottieni l'output del modello\n","            model_output = model(inputs_eval)\n","\n","            # Gestisce il caso in cui il modello restituisca una tupla (es. logits, attention_weights)\n","            # o solo i logits (per il modello LSTM attuale, model_output è già una tupla) (outputs, hidden_state)\n","            if isinstance(model_output, tuple):\n","                outputs_logits_eval = model_output[0] # Prende il primo elemento (i logits)\n","            else: # Il modello restituisce solo un tensore (i logits)\n","                outputs_logits_eval = model_output\n","\n","            # Rimuove dimensioni superflue\n","            squeezed_logits = outputs_logits_eval.squeeze()\n","            # Converte i logits in probabilità\n","            probs_for_batch = torch.sigmoid(squeezed_logits)\n","\n","            # Raccoglie le etichette vere\n","            all_labels_eval.extend(labels_eval.cpu().numpy().tolist())\n","\n","            # Converti le probabilità in NumPy e gestisci il caso 0-D\n","            np_probs_for_batch = probs_for_batch.cpu().numpy()\n","\n","            # Caso in cui probs_for_batch è uno scalare (se batch_size=1 e squeeze() rimuove tutte le dim)\n","            if np_probs_for_batch.ndim == 0:  # È uno scalare (array 0-D)\n","                all_probs_eval.append(float(np_probs_for_batch))\n","            else:\n","                all_probs_eval.extend(np_probs_for_batch.tolist())\n","    return np.array(all_labels_eval), np.array(all_probs_eval)\n","\n","print(\"Funzione get_predictions_and_labels definita.\")"],"metadata":{"id":"AQxJEW_bml1b","executionInfo":{"status":"ok","timestamp":1748652186194,"user_tz":-120,"elapsed":8,"user":{"displayName":"Antonio","userId":"04509716177869496437"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9dbb6ac4-ba9f-42a0-a57d-0a3eb23e889d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Funzione get_predictions_and_labels definita.\n"]}]},{"cell_type":"code","source":["# Caricamento del modello Transformer migliore per la valutazione\n","print(f\"Caricamento del miglior modello Transformer da: {best_model_weights_path}\")\n","model_eval = Transformer(\n","    vocab_sizes=ordered_vocab_sizes,\n","    embedding_dim=EMBEDDING_DIM,\n","    num_continuous_features=num_continuous_features,\n","    d_model=D_MODEL_TRANSFORMER,\n","    nhead=NHEAD_TRANSFORMER,\n","    num_encoder_layers=NUM_ENCODER_LAYERS_TRANSFORMER,\n","    dim_feedforward=DIM_FEEDFORWARD_TRANSFORMER,\n","    output_dim=OUTPUT_DIM,\n","    dropout_rate=DROPOUT_TRANSFORMER,\n","    seq_len=WINDOW_SIZE\n",")\n","model_eval.load_state_dict(torch.load(best_model_weights_path, map_location=DEVICE))\n","model_eval.to(DEVICE)\n","model_eval.eval()\n","print(\"Miglior modello Transformer caricato.\")\n","\n","# Ottieni predizioni una sola volta\n","print(\"\\nOttenimento Predizioni Finali dal Modello Transformer Migliore\")\n","# Carica il modello migliore e ottiene le predizioni (probabilità)\n","# e le etichette vere per il validation e test set\n","val_labels, val_probs = get_predictions_and_labels(model_eval, val_loader, DEVICE)\n","test_labels, test_probs_tf = get_predictions_and_labels(model_eval, test_loader, DEVICE)\n","print(\"Predizioni finali Transformer ottenute.\")\n","\n","# Visualizzazione della distribuzione delle probabilità (VALIDATION SET)\n","# Questa sezione serve a capire come il modello distribuisce le probabilità\n","# per le classi normali e anomale sul validation set. Serve una buona separazione\n","print(\"\\nDistribuzione delle probabilità predette sul Validation Set:\")\n","val_probs_normal = val_probs[val_labels == 0]\n","val_probs_anomalous = val_probs[val_labels == 1]\n","\n","plt.figure(figsize=(12, 6))\n","plt.hist(val_probs_normal, bins=50, alpha=0.7, label=f'Normali (TF Val) (n={len(val_probs_normal)})', color='blue', density=True)\n","if len(val_probs_anomalous) > 0:\n","    plt.hist(val_probs_anomalous, bins=50, alpha=0.7, label=f'Anomale (TF Val) (n={len(val_probs_anomalous)})', color='red', density=True)\n","plt.title('Distribuzione Probabilità (Validation Set) - Transformer')\n","plt.xlabel('Probabilità Predetta di Anomalia'); plt.ylabel('Densità'); plt.legend(); plt.yscale('log'); plt.show()\n","print(f\"Statistiche val_probs_normal (TF): N={len(val_probs_normal)}, Min={np.min(val_probs_normal):.2e}, Max={np.max(val_probs_normal):.2e}, Mean={np.mean(val_probs_normal):.2e}, Median={np.median(val_probs_normal):.2e}\")\n","\n","# Stampa statistiche descrittive delle probabilità per le due classi\n","print(f\"Statistiche val_probs_normal: N={len(val_probs_normal)}, Min={np.min(val_probs_normal):.2e}, Max={np.max(val_probs_normal):.2e}, Mean={np.mean(val_probs_normal):.2e}, Median={np.median(val_probs_normal):.2e}\")\n","if len(val_probs_anomalous) > 0:\n","    print(f\"Statistiche val_probs_anomalous (TF): N={len(val_probs_anomalous)}, Min={np.min(val_probs_anomalous):.2e}, Max={np.max(val_probs_anomalous):.2e}, Mean={np.mean(val_probs_anomalous):.2e}, Median={np.median(val_probs_anomalous):.2e}\")\n","\n","\n","# Determinazione della soglia ottimale con il metodo del percentile (VALIDATION SET)\n","print(\"\\n--- Determinazione Soglia Ottimale con Metodo Percentile (su Validation Set) ---\")\n","percentiles_to_try = [85, 90, 95, 97, 98, 99, 99.5, 99.9]\n","best_f1_val_percentile = -1 # Inizializza a -1.0 per assicurare che qualsiasi F1 valido sia maggiore\n","final_optimal_threshold = 0.5 # Soglia di default se non si trovano anomalie\n","best_percentile_chosen = 0\n","num_anomalies_val = np.sum(val_labels == 1)\n","print(f\"Numero di anomalie effettive nel Validation Set: {num_anomalies_val}\")\n","\n","# Cerca la soglia percentile che massimizza l'F1-score sulla classe anomala\n","if len(val_probs) > 0 and num_anomalies_val > 0:\n","    for p_val in percentiles_to_try:\n","        # Calcola la soglia per il percentile corrente\n","        current_threshold_tf = np.percentile(val_probs, p_val)\n","        # Classifica in base alla soglia\n","        val_preds_tf_p = (val_probs >= current_threshold_tf).astype(int)\n","\n","        precision_tf_p = precision_score(val_labels, val_preds_tf_p, pos_label=1, zero_division=0)\n","        recall_tf_p = recall_score(val_labels, val_preds_tf_p, pos_label=1, zero_division=0)\n","        f1_tf_p = f1_score(val_labels, val_preds_tf_p, pos_label=1, zero_division=0)\n","\n","        print(f\"  Percentile: {p_val:>5}% -> Soglia TF: {current_threshold_tf:.3e} | P: {precision_tf_p:.4f}, R: {recall_tf_p:.4f}, F1: {f1_tf_p:.4f}\")\n","\n","        # Ottimizza per F1-score\n","        if f1_tf_p > best_f1_val_percentile:\n","            best_f1_val_percentile = f1_tf_p\n","            final_optimal_threshold_tf = current_threshold_tf\n","            best_percentile_chosen = p_val\n","    print(f\"\\nMiglior Soglia TF (Percentile) scelta: {final_optimal_threshold_tf:.3e} (dal {best_percentile_chosen}° percentile, F1 su Val TF: {best_f1_val_percentile:.4f})\")\n","elif num_anomalies_val == 0:\n","    print(\"Nessuna anomalia nel validation set, non posso ottimizzare la soglia percentile in modo significativo. Uso default 0.5.\")\n","else: # val_probs_final è vuoto\n","    print(\"val_probs_final è vuoto, non posso calcolare la soglia. Uso default 0.5.\")\n","\n","# Valutazione sul validation set con la soglia scelta\n","print(f\"\\n--- Performance sul Validation Set con Soglia Finale ({final_optimal_threshold:.2e}) ---\")\n","val_predicted_labels_tf = (val_probs >= final_optimal_threshold_tf).astype(int)\n","print(classification_report(val_labels, val_predicted_labels_tf, target_names=['Normale (0)', 'Anomalo (1)'], zero_division=0))\n","cm_val_tf = confusion_matrix(val_labels, val_predicted_labels_tf)\n","plt.figure(figsize=(6,4)); sns.heatmap(cm_val_tf, annot=True, fmt='d', cmap='BuGn', xticklabels=['Pred. Normale', 'Pred. Anomalo'], yticklabels=['Vero Normale', 'Vero Anomalo'])\n","plt.title(f'Validation Set TF - CM (Soglia={final_optimal_threshold_tf:.2e})'); plt.xlabel('Predetta'); plt.ylabel('Vera'); plt.show()\n","\n","# Valutazione finale sul test set con la soglia scelta\n","print(f\"\\nPerformance Finale Transformer su Test Set con Soglia ({final_optimal_threshold_tf:.3e})\")\n","test_predicted_labels_tf = (test_probs_tf >= final_optimal_threshold_tf).astype(int)\n","\n","print(\"\\nClassification Report completo sul Test Set:\")\n","print(classification_report(test_labels, test_predicted_labels_tf, target_names=['Normale (0)', 'Anomalo (1)'], zero_division=0))\n","\n","cm_test = confusion_matrix(test_labels, test_predicted_labels_tf)\n","plt.figure(figsize=(6,4)); sns.heatmap(cm_test, annot=True, fmt='d', cmap='BuGn', xticklabels=['Pred. Normale', 'Pred. Anomalo'], yticklabels=['Vero Normale', 'Vero Anomalo'])\n","plt.title(f'Test Set TF - CM (Soglia={final_optimal_threshold_tf:.2e})'); plt.xlabel('Predetta'); plt.ylabel('Vera'); plt.show()\n","\n","# Conteggio finale dei veri positivi sul test set\n","num_anomalies_test = np.sum(test_labels == 1)\n","true_positives_test = cm_test[1, 1] if num_anomalies_test > 0 and cm_test.shape == (2,2) else 0\n","print(f\"Numero di anomalie effettive nel Test Set: {num_anomalies_test}\")\n","if num_anomalies_test > 0:\n","    print(f\"Anomalie correttamente identificate (TP) nel Test Set: {true_positives_test} su {num_anomalies_test}\")"],"metadata":{"id":"eZmWFH9Qa3BT"},"execution_count":null,"outputs":[]}]}